{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import TypedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from constants import DATA_DIR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from astrofit.utils import AsteroidLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "asteroid_loader = AsteroidLoader(DATA_DIR)\n",
    "\n",
    "ASTEROIDS_FREQ_DATA_PATH = DATA_DIR / \"asteroids_freq_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ASTEROIDS_FREQ_DATA_PATH, \"r\") as f:\n",
    "    asteroids_freq_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config, asteroids_data = asteroids_freq_data[\"config\"], asteroids_freq_data[\"asteroids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 2662 asteroids (40.41% failed)\n"
     ]
    }
   ],
   "source": [
    "filtered_data = {name: data for name, data in asteroids_data.items() if not data[\"is_failed\"]}\n",
    "print(f\"Filtered {len(filtered_data)} asteroids ({100*(len(asteroids_data) - len(filtered_data)) / len(asteroids_data):.2f}% failed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsteroidData(TypedDict):\n",
    "    is_failed: bool\n",
    "    reason: str | None\n",
    "    period: float\n",
    "    processing_time: float\n",
    "    freq_features: list[list]  # 1 - 4 sequences of 50 floats (freqs) from 0 to 12\n",
    "    pow_features: list[list]  # 1 - 4 sequences of 50 floats (pows) from 0 to 1 (the same shape as freq_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = {name: AsteroidData(**data) for name, data in filtered_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped 1652 asteroids\n"
     ]
    }
   ],
   "source": [
    "no_clipped = 0\n",
    "for key in filtered_data:\n",
    "    data = filtered_data[key]\n",
    "    if len(data[\"freq_features\"]) > 1:\n",
    "        # Clip to just the first sequence\n",
    "        data[\"freq_features\"] = [data[\"freq_features\"][0]]\n",
    "        data[\"pow_features\"] = [data[\"pow_features\"][0]]\n",
    "\n",
    "        no_clipped += 1\n",
    "\n",
    "print(f\"Clipped {no_clipped} asteroids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2129 asteroids (79.98)\n",
      "Validation: 357 asteroids (13.41)\n",
      "Test: 176 asteroids (6.61)\n"
     ]
    }
   ],
   "source": [
    "train_keys, val_test_keys = train_test_split(list(filtered_data.keys()), test_size=0.2, random_state=884288)\n",
    "val_keys, test_keys = train_test_split(val_test_keys, test_size=0.33, random_state=884288)\n",
    "\n",
    "print(f\"Train: {len(train_keys)} asteroids ({100 * len(train_keys) / len(filtered_data):.2f})\")\n",
    "print(f\"Validation: {len(val_keys)} asteroids ({100*len(val_keys) / len(filtered_data):.2f})\")\n",
    "print(f\"Test: {len(test_keys)} asteroids ({100 * len(test_keys) / len(filtered_data):.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = (\n",
    "    {key: filtered_data[key] for key in train_keys},\n",
    "    {key: filtered_data[key] for key in val_keys},\n",
    "    {key: filtered_data[key] for key in test_keys},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data_set: dict[str, AsteroidData]) -> np.ndarray:\n",
    "    freqs = np.array([data[\"freq_features\"] for data in data_set.values()])\n",
    "    powers = np.array([data[\"pow_features\"] for data in data_set.values()])\n",
    "    return np.stack([freqs, powers], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = extract_features(train_set)\n",
    "val_features = extract_features(val_set)\n",
    "test_features = extract_features(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features_scaled: np.ndarray = scaler.fit_transform(\n",
    "    train_features.reshape(-1, train_features.shape[-1]),\n",
    ").reshape(train_features.shape)\n",
    "\n",
    "# Transform validation and test data using the same scaler\n",
    "val_features_scaled: np.ndarray = scaler.transform(\n",
    "    val_features.reshape(-1, val_features.shape[-1]),\n",
    ").reshape(val_features.shape)  # type: ignore\n",
    "\n",
    "test_features_scaled: np.ndarray = scaler.transform(\n",
    "    test_features.reshape(-1, test_features.shape[-1]),\n",
    ").reshape(test_features.shape)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_periods = np.array([sample[\"period\"] for sample in train_set.values()])\n",
    "val_periods = np.array([sample[\"period\"] for sample in val_set.values()])\n",
    "test_periods = np.array([sample[\"period\"] for sample in test_set.values()])\n",
    "\n",
    "train_freqs = 24 / train_periods\n",
    "val_freqs = 24 / val_periods\n",
    "test_freqs = 24 / test_periods\n",
    "\n",
    "# Standardize target frequencies using training set statistics\n",
    "target_scaler = StandardScaler()\n",
    "train_freqs_scaled: np.ndarray = target_scaler.fit_transform(train_freqs.reshape(-1, 1)).flatten()\n",
    "val_freqs_scaled: np.ndarray = target_scaler.transform(val_freqs.reshape(-1, 1)).flatten()  # type: ignore\n",
    "test_freqs_scaled: np.ndarray = target_scaler.transform(test_freqs.reshape(-1, 1)).flatten()  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsteroidDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray, targets: np.ndarray):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AsteroidDataset(train_features_scaled, train_freqs_scaled)\n",
    "val_dataset = AsteroidDataset(val_features_scaled, val_freqs_scaled)\n",
    "test_dataset = AsteroidDataset(test_features_scaled, test_freqs_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 50, 2]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "data, targets = next(iter(train_loader))\n",
    "print(data.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsteroidPeriodPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AsteroidPeriodPredictor, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(2, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_sessions, num_freq, num_features = x.shape\n",
    "\n",
    "        # Reshape to (batch_size, num_features, num_freq)\n",
    "        x = x.squeeze(1)  # Remove the num_sessions dimension (which is 1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(batch_size, -1)  # Flatten: (batch_size, 128)\n",
    "        x = self.fc(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AsteroidPeriodPredictor(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv1d(2, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AsteroidPeriodPredictor().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=1000, patience=50):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=20)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_targets = [], []\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(outputs.cpu().detach().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_r2 = r2_score(train_targets, train_preds)\n",
    "        train_mae = mean_absolute_error(train_targets, train_preds)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_targets = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, targets in val_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                outputs = model(data)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_r2 = r2_score(val_targets, val_preds)\n",
    "        val_mae = mean_absolute_error(val_targets, val_preds)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"{epoch + 1}/{num_epochs} - \", end=\"\")\n",
    "        print(f\"Train-loss: {train_loss:.4f}, Train-R2: {train_r2:.4f}, Train-MAE: {train_mae:.4f}\", end=\"\\t- \")\n",
    "        print(f\"Val-loss: {val_loss:.4f}, Val-R2: {val_r2:.4f}, Val-MAE: {val_mae:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "\n",
    "    print(f\"Best validation loss: {best_val_loss}\")\n",
    "\n",
    "    # Load the best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1000 - Train-loss: 0.9748, Train-R2: 0.0248, Train-MAE: 0.7890\t- Val-loss: 1.1157, Val-R2: 0.0187, Val-MAE: 0.7909\n",
      "2/1000 - Train-loss: 0.9581, Train-R2: 0.0464, Train-MAE: 0.7772\t- Val-loss: 1.0923, Val-R2: 0.0237, Val-MAE: 0.8053\n",
      "3/1000 - Train-loss: 0.9312, Train-R2: 0.0680, Train-MAE: 0.7679\t- Val-loss: 1.0816, Val-R2: 0.0463, Val-MAE: 0.7883\n",
      "4/1000 - Train-loss: 0.9276, Train-R2: 0.0745, Train-MAE: 0.7636\t- Val-loss: 1.1136, Val-R2: 0.0160, Val-MAE: 0.8017\n",
      "5/1000 - Train-loss: 0.9310, Train-R2: 0.0718, Train-MAE: 0.7695\t- Val-loss: 1.1254, Val-R2: -0.0060, Val-MAE: 0.8099\n",
      "6/1000 - Train-loss: 0.9103, Train-R2: 0.0891, Train-MAE: 0.7607\t- Val-loss: 1.1232, Val-R2: -0.0061, Val-MAE: 0.8082\n",
      "7/1000 - Train-loss: 0.9146, Train-R2: 0.0915, Train-MAE: 0.7581\t- Val-loss: 1.1248, Val-R2: -0.0124, Val-MAE: 0.8017\n",
      "8/1000 - Train-loss: 0.9128, Train-R2: 0.0872, Train-MAE: 0.7585\t- Val-loss: 1.1017, Val-R2: 0.0201, Val-MAE: 0.7889\n",
      "9/1000 - Train-loss: 0.9023, Train-R2: 0.1014, Train-MAE: 0.7524\t- Val-loss: 1.0861, Val-R2: 0.0260, Val-MAE: 0.8029\n",
      "10/1000 - Train-loss: 0.8933, Train-R2: 0.1038, Train-MAE: 0.7492\t- Val-loss: 1.0669, Val-R2: 0.0476, Val-MAE: 0.7883\n",
      "11/1000 - Train-loss: 0.9044, Train-R2: 0.1015, Train-MAE: 0.7531\t- Val-loss: 1.1026, Val-R2: 0.0191, Val-MAE: 0.7899\n",
      "12/1000 - Train-loss: 0.8976, Train-R2: 0.1028, Train-MAE: 0.7496\t- Val-loss: 1.1166, Val-R2: 0.0278, Val-MAE: 0.7959\n",
      "13/1000 - Train-loss: 0.9027, Train-R2: 0.0979, Train-MAE: 0.7577\t- Val-loss: 1.0781, Val-R2: 0.0392, Val-MAE: 0.7937\n",
      "14/1000 - Train-loss: 0.8918, Train-R2: 0.1061, Train-MAE: 0.7478\t- Val-loss: 1.0980, Val-R2: 0.0143, Val-MAE: 0.7848\n",
      "15/1000 - Train-loss: 0.8907, Train-R2: 0.1109, Train-MAE: 0.7464\t- Val-loss: 1.0632, Val-R2: 0.0589, Val-MAE: 0.7723\n",
      "16/1000 - Train-loss: 0.8885, Train-R2: 0.1152, Train-MAE: 0.7450\t- Val-loss: 1.0983, Val-R2: 0.0251, Val-MAE: 0.7797\n",
      "17/1000 - Train-loss: 0.8838, Train-R2: 0.1161, Train-MAE: 0.7463\t- Val-loss: 1.0949, Val-R2: 0.0270, Val-MAE: 0.7826\n",
      "18/1000 - Train-loss: 0.8731, Train-R2: 0.1244, Train-MAE: 0.7389\t- Val-loss: 1.0796, Val-R2: 0.0353, Val-MAE: 0.7734\n",
      "19/1000 - Train-loss: 0.8729, Train-R2: 0.1291, Train-MAE: 0.7358\t- Val-loss: 1.1513, Val-R2: -0.0149, Val-MAE: 0.7980\n",
      "20/1000 - Train-loss: 0.8843, Train-R2: 0.1177, Train-MAE: 0.7444\t- Val-loss: 1.0587, Val-R2: 0.0652, Val-MAE: 0.7596\n",
      "21/1000 - Train-loss: 0.8638, Train-R2: 0.1423, Train-MAE: 0.7297\t- Val-loss: 1.0824, Val-R2: 0.0462, Val-MAE: 0.7634\n",
      "22/1000 - Train-loss: 0.8703, Train-R2: 0.1295, Train-MAE: 0.7353\t- Val-loss: 1.0667, Val-R2: 0.0498, Val-MAE: 0.7665\n",
      "23/1000 - Train-loss: 0.8560, Train-R2: 0.1451, Train-MAE: 0.7255\t- Val-loss: 1.0784, Val-R2: 0.0440, Val-MAE: 0.7785\n",
      "24/1000 - Train-loss: 0.8462, Train-R2: 0.1520, Train-MAE: 0.7284\t- Val-loss: 1.0725, Val-R2: 0.0540, Val-MAE: 0.7681\n",
      "25/1000 - Train-loss: 0.8329, Train-R2: 0.1640, Train-MAE: 0.7172\t- Val-loss: 1.0659, Val-R2: 0.0584, Val-MAE: 0.7715\n",
      "26/1000 - Train-loss: 0.8220, Train-R2: 0.1750, Train-MAE: 0.7122\t- Val-loss: 1.0090, Val-R2: 0.1228, Val-MAE: 0.7445\n",
      "27/1000 - Train-loss: 0.8424, Train-R2: 0.1573, Train-MAE: 0.7192\t- Val-loss: 1.0462, Val-R2: 0.0661, Val-MAE: 0.7467\n",
      "28/1000 - Train-loss: 0.8279, Train-R2: 0.1730, Train-MAE: 0.7148\t- Val-loss: 1.0566, Val-R2: 0.0536, Val-MAE: 0.7485\n",
      "29/1000 - Train-loss: 0.8083, Train-R2: 0.1919, Train-MAE: 0.7025\t- Val-loss: 1.0045, Val-R2: 0.1029, Val-MAE: 0.7518\n",
      "30/1000 - Train-loss: 0.8074, Train-R2: 0.1895, Train-MAE: 0.7006\t- Val-loss: 1.0411, Val-R2: 0.0677, Val-MAE: 0.7615\n",
      "31/1000 - Train-loss: 0.8374, Train-R2: 0.1610, Train-MAE: 0.7115\t- Val-loss: 1.1049, Val-R2: 0.0512, Val-MAE: 0.7735\n",
      "32/1000 - Train-loss: 0.8058, Train-R2: 0.1953, Train-MAE: 0.6960\t- Val-loss: 1.0234, Val-R2: 0.0911, Val-MAE: 0.7594\n",
      "33/1000 - Train-loss: 0.8014, Train-R2: 0.2030, Train-MAE: 0.6973\t- Val-loss: 1.0051, Val-R2: 0.1028, Val-MAE: 0.7381\n",
      "34/1000 - Train-loss: 0.8051, Train-R2: 0.1928, Train-MAE: 0.6997\t- Val-loss: 0.9894, Val-R2: 0.1153, Val-MAE: 0.7360\n",
      "35/1000 - Train-loss: 0.8118, Train-R2: 0.1874, Train-MAE: 0.7019\t- Val-loss: 1.0493, Val-R2: 0.0632, Val-MAE: 0.7509\n",
      "36/1000 - Train-loss: 0.7827, Train-R2: 0.2152, Train-MAE: 0.6930\t- Val-loss: 0.9969, Val-R2: 0.1034, Val-MAE: 0.7550\n",
      "37/1000 - Train-loss: 0.7672, Train-R2: 0.2323, Train-MAE: 0.6846\t- Val-loss: 1.0003, Val-R2: 0.0916, Val-MAE: 0.7416\n",
      "38/1000 - Train-loss: 0.7925, Train-R2: 0.2124, Train-MAE: 0.6888\t- Val-loss: 1.0033, Val-R2: 0.1088, Val-MAE: 0.7417\n",
      "39/1000 - Train-loss: 0.7668, Train-R2: 0.2311, Train-MAE: 0.6770\t- Val-loss: 1.0431, Val-R2: 0.0876, Val-MAE: 0.7548\n",
      "40/1000 - Train-loss: 0.7828, Train-R2: 0.2158, Train-MAE: 0.6848\t- Val-loss: 1.0357, Val-R2: 0.0838, Val-MAE: 0.7488\n",
      "41/1000 - Train-loss: 0.7583, Train-R2: 0.2398, Train-MAE: 0.6772\t- Val-loss: 1.0758, Val-R2: 0.0130, Val-MAE: 0.7828\n",
      "42/1000 - Train-loss: 0.7894, Train-R2: 0.2086, Train-MAE: 0.6912\t- Val-loss: 0.9934, Val-R2: 0.1067, Val-MAE: 0.7473\n",
      "43/1000 - Train-loss: 0.7581, Train-R2: 0.2412, Train-MAE: 0.6723\t- Val-loss: 1.1277, Val-R2: 0.0172, Val-MAE: 0.7902\n",
      "44/1000 - Train-loss: 0.7704, Train-R2: 0.2298, Train-MAE: 0.6804\t- Val-loss: 1.0444, Val-R2: 0.0786, Val-MAE: 0.7410\n",
      "45/1000 - Train-loss: 0.7493, Train-R2: 0.2519, Train-MAE: 0.6680\t- Val-loss: 1.1017, Val-R2: 0.0221, Val-MAE: 0.7434\n",
      "46/1000 - Train-loss: 0.7592, Train-R2: 0.2405, Train-MAE: 0.6752\t- Val-loss: 0.9904, Val-R2: 0.1115, Val-MAE: 0.7507\n",
      "47/1000 - Train-loss: 0.7167, Train-R2: 0.2835, Train-MAE: 0.6521\t- Val-loss: 1.1645, Val-R2: -0.0688, Val-MAE: 0.7724\n",
      "48/1000 - Train-loss: 0.7693, Train-R2: 0.2313, Train-MAE: 0.6761\t- Val-loss: 1.0404, Val-R2: 0.0589, Val-MAE: 0.7560\n",
      "49/1000 - Train-loss: 0.7349, Train-R2: 0.2628, Train-MAE: 0.6616\t- Val-loss: 1.0130, Val-R2: 0.0977, Val-MAE: 0.7445\n",
      "50/1000 - Train-loss: 0.7373, Train-R2: 0.2617, Train-MAE: 0.6659\t- Val-loss: 1.0051, Val-R2: 0.0919, Val-MAE: 0.7500\n",
      "51/1000 - Train-loss: 0.7326, Train-R2: 0.2673, Train-MAE: 0.6608\t- Val-loss: 1.0716, Val-R2: 0.0298, Val-MAE: 0.7462\n",
      "52/1000 - Train-loss: 0.7215, Train-R2: 0.2806, Train-MAE: 0.6558\t- Val-loss: 1.0254, Val-R2: 0.0884, Val-MAE: 0.7348\n",
      "53/1000 - Train-loss: 0.7398, Train-R2: 0.2647, Train-MAE: 0.6666\t- Val-loss: 1.0447, Val-R2: 0.0676, Val-MAE: 0.7420\n",
      "54/1000 - Train-loss: 0.7019, Train-R2: 0.2991, Train-MAE: 0.6439\t- Val-loss: 1.1661, Val-R2: -0.0214, Val-MAE: 0.8106\n",
      "55/1000 - Train-loss: 0.7236, Train-R2: 0.2769, Train-MAE: 0.6603\t- Val-loss: 1.0472, Val-R2: 0.0529, Val-MAE: 0.7492\n",
      "56/1000 - Train-loss: 0.6743, Train-R2: 0.3243, Train-MAE: 0.6314\t- Val-loss: 1.0520, Val-R2: 0.0479, Val-MAE: 0.7551\n",
      "57/1000 - Train-loss: 0.6791, Train-R2: 0.3197, Train-MAE: 0.6373\t- Val-loss: 1.0544, Val-R2: 0.0523, Val-MAE: 0.7433\n",
      "58/1000 - Train-loss: 0.6489, Train-R2: 0.3509, Train-MAE: 0.6177\t- Val-loss: 1.0361, Val-R2: 0.0553, Val-MAE: 0.7407\n",
      "59/1000 - Train-loss: 0.6377, Train-R2: 0.3616, Train-MAE: 0.6111\t- Val-loss: 1.0641, Val-R2: 0.0125, Val-MAE: 0.7587\n",
      "60/1000 - Train-loss: 0.6431, Train-R2: 0.3554, Train-MAE: 0.6178\t- Val-loss: 1.0379, Val-R2: 0.0570, Val-MAE: 0.7587\n",
      "61/1000 - Train-loss: 0.6418, Train-R2: 0.3591, Train-MAE: 0.6109\t- Val-loss: 1.0383, Val-R2: 0.0634, Val-MAE: 0.7580\n",
      "62/1000 - Train-loss: 0.6329, Train-R2: 0.3683, Train-MAE: 0.6105\t- Val-loss: 1.0900, Val-R2: -0.0010, Val-MAE: 0.7606\n",
      "63/1000 - Train-loss: 0.6074, Train-R2: 0.3891, Train-MAE: 0.5944\t- Val-loss: 1.0732, Val-R2: 0.0446, Val-MAE: 0.7650\n",
      "64/1000 - Train-loss: 0.6205, Train-R2: 0.3795, Train-MAE: 0.6027\t- Val-loss: 1.0206, Val-R2: 0.0514, Val-MAE: 0.7505\n",
      "65/1000 - Train-loss: 0.6393, Train-R2: 0.3589, Train-MAE: 0.6130\t- Val-loss: 1.0650, Val-R2: 0.0241, Val-MAE: 0.7566\n",
      "66/1000 - Train-loss: 0.6093, Train-R2: 0.3895, Train-MAE: 0.5968\t- Val-loss: 1.0412, Val-R2: 0.0292, Val-MAE: 0.7737\n",
      "67/1000 - Train-loss: 0.6107, Train-R2: 0.3918, Train-MAE: 0.5941\t- Val-loss: 1.0966, Val-R2: 0.0105, Val-MAE: 0.7612\n",
      "68/1000 - Train-loss: 0.6075, Train-R2: 0.3914, Train-MAE: 0.5944\t- Val-loss: 1.0291, Val-R2: 0.0609, Val-MAE: 0.7491\n",
      "69/1000 - Train-loss: 0.5993, Train-R2: 0.4004, Train-MAE: 0.5912\t- Val-loss: 1.1804, Val-R2: -0.0271, Val-MAE: 0.7925\n",
      "70/1000 - Train-loss: 0.6081, Train-R2: 0.3906, Train-MAE: 0.5948\t- Val-loss: 1.1319, Val-R2: -0.0262, Val-MAE: 0.7692\n",
      "71/1000 - Train-loss: 0.5884, Train-R2: 0.4138, Train-MAE: 0.5807\t- Val-loss: 1.2060, Val-R2: -0.1086, Val-MAE: 0.7869\n",
      "72/1000 - Train-loss: 0.5954, Train-R2: 0.4042, Train-MAE: 0.5878\t- Val-loss: 1.2257, Val-R2: -0.0636, Val-MAE: 0.8166\n",
      "73/1000 - Train-loss: 0.5742, Train-R2: 0.4241, Train-MAE: 0.5827\t- Val-loss: 1.1442, Val-R2: -0.0476, Val-MAE: 0.7929\n",
      "74/1000 - Train-loss: 0.5949, Train-R2: 0.4038, Train-MAE: 0.5869\t- Val-loss: 1.2520, Val-R2: -0.1138, Val-MAE: 0.8388\n",
      "75/1000 - Train-loss: 0.5878, Train-R2: 0.4123, Train-MAE: 0.5846\t- Val-loss: 1.1349, Val-R2: -0.0508, Val-MAE: 0.7809\n",
      "76/1000 - Train-loss: 0.5651, Train-R2: 0.4334, Train-MAE: 0.5725\t- Val-loss: 1.1425, Val-R2: -0.0337, Val-MAE: 0.7767\n",
      "77/1000 - Train-loss: 0.5578, Train-R2: 0.4484, Train-MAE: 0.5564\t- Val-loss: 1.1397, Val-R2: -0.0317, Val-MAE: 0.7959\n",
      "78/1000 - Train-loss: 0.5622, Train-R2: 0.4426, Train-MAE: 0.5646\t- Val-loss: 1.1302, Val-R2: -0.0406, Val-MAE: 0.7701\n",
      "79/1000 - Train-loss: 0.5508, Train-R2: 0.4557, Train-MAE: 0.5553\t- Val-loss: 1.0895, Val-R2: 0.0195, Val-MAE: 0.7621\n",
      "80/1000 - Train-loss: 0.5460, Train-R2: 0.4612, Train-MAE: 0.5541\t- Val-loss: 1.1444, Val-R2: -0.0314, Val-MAE: 0.7780\n",
      "81/1000 - Train-loss: 0.5384, Train-R2: 0.4643, Train-MAE: 0.5500\t- Val-loss: 1.1119, Val-R2: -0.0094, Val-MAE: 0.7754\n",
      "82/1000 - Train-loss: 0.5263, Train-R2: 0.4761, Train-MAE: 0.5447\t- Val-loss: 1.1248, Val-R2: -0.0134, Val-MAE: 0.7712\n",
      "83/1000 - Train-loss: 0.5239, Train-R2: 0.4748, Train-MAE: 0.5458\t- Val-loss: 1.1229, Val-R2: -0.0174, Val-MAE: 0.7799\n",
      "84/1000 - Train-loss: 0.5241, Train-R2: 0.4824, Train-MAE: 0.5368\t- Val-loss: 1.1555, Val-R2: -0.0345, Val-MAE: 0.7925\n",
      "85/1000 - Train-loss: 0.5220, Train-R2: 0.4782, Train-MAE: 0.5503\t- Val-loss: 1.1340, Val-R2: -0.0218, Val-MAE: 0.7839\n",
      "86/1000 - Train-loss: 0.5223, Train-R2: 0.4800, Train-MAE: 0.5408\t- Val-loss: 1.2006, Val-R2: -0.0546, Val-MAE: 0.8009\n",
      "87/1000 - Train-loss: 0.5199, Train-R2: 0.4814, Train-MAE: 0.5430\t- Val-loss: 1.2142, Val-R2: -0.0860, Val-MAE: 0.8181\n",
      "88/1000 - Train-loss: 0.5033, Train-R2: 0.4949, Train-MAE: 0.5340\t- Val-loss: 1.1167, Val-R2: -0.0151, Val-MAE: 0.7915\n",
      "89/1000 - Train-loss: 0.5073, Train-R2: 0.4912, Train-MAE: 0.5336\t- Val-loss: 1.2042, Val-R2: -0.1004, Val-MAE: 0.8055\n",
      "90/1000 - Train-loss: 0.5215, Train-R2: 0.4861, Train-MAE: 0.5381\t- Val-loss: 1.2019, Val-R2: -0.0810, Val-MAE: 0.8146\n",
      "91/1000 - Train-loss: 0.5042, Train-R2: 0.4957, Train-MAE: 0.5311\t- Val-loss: 1.1605, Val-R2: -0.0544, Val-MAE: 0.8126\n",
      "92/1000 - Train-loss: 0.5206, Train-R2: 0.4778, Train-MAE: 0.5466\t- Val-loss: 1.1615, Val-R2: -0.0591, Val-MAE: 0.7871\n",
      "93/1000 - Train-loss: 0.5011, Train-R2: 0.4998, Train-MAE: 0.5314\t- Val-loss: 1.1058, Val-R2: -0.0031, Val-MAE: 0.7759\n",
      "94/1000 - Train-loss: 0.4972, Train-R2: 0.5032, Train-MAE: 0.5251\t- Val-loss: 1.1857, Val-R2: -0.0853, Val-MAE: 0.8022\n",
      "95/1000 - Train-loss: 0.5163, Train-R2: 0.4817, Train-MAE: 0.5382\t- Val-loss: 1.1641, Val-R2: -0.0735, Val-MAE: 0.7986\n",
      "96/1000 - Train-loss: 0.5055, Train-R2: 0.4953, Train-MAE: 0.5352\t- Val-loss: 1.1277, Val-R2: -0.0572, Val-MAE: 0.7770\n",
      "97/1000 - Train-loss: 0.5127, Train-R2: 0.4884, Train-MAE: 0.5327\t- Val-loss: 1.2280, Val-R2: -0.1045, Val-MAE: 0.8171\n",
      "98/1000 - Train-loss: 0.4820, Train-R2: 0.5203, Train-MAE: 0.5164\t- Val-loss: 1.1720, Val-R2: -0.0781, Val-MAE: 0.7986\n",
      "99/1000 - Train-loss: 0.4807, Train-R2: 0.5179, Train-MAE: 0.5183\t- Val-loss: 1.1798, Val-R2: -0.0714, Val-MAE: 0.7932\n",
      "100/1000 - Train-loss: 0.4719, Train-R2: 0.5278, Train-MAE: 0.5098\t- Val-loss: 1.2236, Val-R2: -0.1285, Val-MAE: 0.8110\n",
      "101/1000 - Train-loss: 0.4759, Train-R2: 0.5297, Train-MAE: 0.5072\t- Val-loss: 1.1951, Val-R2: -0.0987, Val-MAE: 0.7997\n",
      "102/1000 - Train-loss: 0.4730, Train-R2: 0.5247, Train-MAE: 0.5153\t- Val-loss: 1.1773, Val-R2: -0.0902, Val-MAE: 0.8010\n",
      "103/1000 - Train-loss: 0.4694, Train-R2: 0.5333, Train-MAE: 0.5040\t- Val-loss: 1.1939, Val-R2: -0.1040, Val-MAE: 0.8128\n",
      "104/1000 - Train-loss: 0.4639, Train-R2: 0.5368, Train-MAE: 0.5034\t- Val-loss: 1.1801, Val-R2: -0.0748, Val-MAE: 0.7990\n",
      "105/1000 - Train-loss: 0.4630, Train-R2: 0.5365, Train-MAE: 0.5022\t- Val-loss: 1.1995, Val-R2: -0.1037, Val-MAE: 0.8116\n",
      "106/1000 - Train-loss: 0.4783, Train-R2: 0.5213, Train-MAE: 0.5144\t- Val-loss: 1.1877, Val-R2: -0.0822, Val-MAE: 0.7986\n",
      "107/1000 - Train-loss: 0.4732, Train-R2: 0.5260, Train-MAE: 0.5130\t- Val-loss: 1.2290, Val-R2: -0.1138, Val-MAE: 0.8137\n",
      "108/1000 - Train-loss: 0.4680, Train-R2: 0.5297, Train-MAE: 0.5082\t- Val-loss: 1.1794, Val-R2: -0.0690, Val-MAE: 0.8073\n",
      "109/1000 - Train-loss: 0.4652, Train-R2: 0.5336, Train-MAE: 0.5024\t- Val-loss: 1.2076, Val-R2: -0.1110, Val-MAE: 0.8043\n",
      "110/1000 - Train-loss: 0.4671, Train-R2: 0.5311, Train-MAE: 0.5065\t- Val-loss: 1.2131, Val-R2: -0.1132, Val-MAE: 0.8031\n",
      "111/1000 - Train-loss: 0.4610, Train-R2: 0.5424, Train-MAE: 0.4982\t- Val-loss: 1.1914, Val-R2: -0.0803, Val-MAE: 0.7998\n",
      "112/1000 - Train-loss: 0.4528, Train-R2: 0.5457, Train-MAE: 0.5012\t- Val-loss: 1.1878, Val-R2: -0.0902, Val-MAE: 0.8041\n",
      "113/1000 - Train-loss: 0.4604, Train-R2: 0.5383, Train-MAE: 0.5037\t- Val-loss: 1.2052, Val-R2: -0.1122, Val-MAE: 0.8082\n",
      "114/1000 - Train-loss: 0.4555, Train-R2: 0.5433, Train-MAE: 0.4972\t- Val-loss: 1.2112, Val-R2: -0.1276, Val-MAE: 0.7999\n",
      "115/1000 - Train-loss: 0.4581, Train-R2: 0.5422, Train-MAE: 0.4955\t- Val-loss: 1.2168, Val-R2: -0.1350, Val-MAE: 0.8158\n",
      "116/1000 - Train-loss: 0.4709, Train-R2: 0.5315, Train-MAE: 0.5096\t- Val-loss: 1.1775, Val-R2: -0.0754, Val-MAE: 0.7990\n",
      "117/1000 - Train-loss: 0.4524, Train-R2: 0.5463, Train-MAE: 0.4966\t- Val-loss: 1.1866, Val-R2: -0.0956, Val-MAE: 0.8071\n",
      "118/1000 - Train-loss: 0.4596, Train-R2: 0.5406, Train-MAE: 0.5019\t- Val-loss: 1.2026, Val-R2: -0.1264, Val-MAE: 0.8090\n",
      "119/1000 - Train-loss: 0.4523, Train-R2: 0.5479, Train-MAE: 0.4948\t- Val-loss: 1.1850, Val-R2: -0.1027, Val-MAE: 0.8054\n",
      "120/1000 - Train-loss: 0.4484, Train-R2: 0.5498, Train-MAE: 0.4930\t- Val-loss: 1.1947, Val-R2: -0.1037, Val-MAE: 0.8044\n",
      "121/1000 - Train-loss: 0.4501, Train-R2: 0.5540, Train-MAE: 0.4875\t- Val-loss: 1.1903, Val-R2: -0.0995, Val-MAE: 0.7987\n",
      "122/1000 - Train-loss: 0.4623, Train-R2: 0.5376, Train-MAE: 0.5026\t- Val-loss: 1.1814, Val-R2: -0.0977, Val-MAE: 0.8066\n",
      "123/1000 - Train-loss: 0.4401, Train-R2: 0.5601, Train-MAE: 0.4901\t- Val-loss: 1.1645, Val-R2: -0.0957, Val-MAE: 0.8015\n",
      "124/1000 - Train-loss: 0.4325, Train-R2: 0.5678, Train-MAE: 0.4821\t- Val-loss: 1.2118, Val-R2: -0.1140, Val-MAE: 0.8130\n",
      "125/1000 - Train-loss: 0.4403, Train-R2: 0.5614, Train-MAE: 0.4897\t- Val-loss: 1.2162, Val-R2: -0.1296, Val-MAE: 0.8148\n",
      "126/1000 - Train-loss: 0.4564, Train-R2: 0.5438, Train-MAE: 0.4954\t- Val-loss: 1.2017, Val-R2: -0.1044, Val-MAE: 0.8060\n",
      "127/1000 - Train-loss: 0.4460, Train-R2: 0.5543, Train-MAE: 0.4882\t- Val-loss: 1.1799, Val-R2: -0.0856, Val-MAE: 0.8022\n",
      "128/1000 - Train-loss: 0.4351, Train-R2: 0.5642, Train-MAE: 0.4831\t- Val-loss: 1.1956, Val-R2: -0.0966, Val-MAE: 0.8073\n",
      "129/1000 - Train-loss: 0.4553, Train-R2: 0.5528, Train-MAE: 0.4880\t- Val-loss: 1.2189, Val-R2: -0.1285, Val-MAE: 0.8111\n",
      "130/1000 - Train-loss: 0.4303, Train-R2: 0.5686, Train-MAE: 0.4756\t- Val-loss: 1.2407, Val-R2: -0.1550, Val-MAE: 0.8084\n",
      "131/1000 - Train-loss: 0.4445, Train-R2: 0.5544, Train-MAE: 0.4922\t- Val-loss: 1.2217, Val-R2: -0.1314, Val-MAE: 0.8047\n",
      "132/1000 - Train-loss: 0.4388, Train-R2: 0.5607, Train-MAE: 0.4937\t- Val-loss: 1.1961, Val-R2: -0.1053, Val-MAE: 0.8038\n",
      "133/1000 - Train-loss: 0.4473, Train-R2: 0.5546, Train-MAE: 0.4875\t- Val-loss: 1.1969, Val-R2: -0.0950, Val-MAE: 0.8033\n",
      "134/1000 - Train-loss: 0.4391, Train-R2: 0.5604, Train-MAE: 0.4854\t- Val-loss: 1.2157, Val-R2: -0.1309, Val-MAE: 0.8153\n",
      "\n",
      "Early stopping triggered after 134 epochs\n",
      "Best validation loss: 0.9893983056147894\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, val_loader, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, target_scaler):\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in tqdm(test_loader, desc=\"Evaluating on test set\"):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            test_preds.extend(outputs.cpu().numpy())\n",
    "            test_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    test_preds = np.array(test_preds)\n",
    "    test_targets = np.array(test_targets)\n",
    "\n",
    "    # Inverse transform the predictions and targets if they were scaled\n",
    "    if target_scaler is not None:\n",
    "        test_preds = target_scaler.inverse_transform(test_preds.reshape(-1, 1)).flatten()\n",
    "        test_targets = target_scaler.inverse_transform(test_targets.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(test_targets, test_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(test_targets, test_preds)\n",
    "    r2 = r2_score(test_targets, test_preds)\n",
    "\n",
    "    # Convert frequencies back to periods\n",
    "    test_periods_pred = 24 / test_preds\n",
    "    test_periods_true = 24 / test_targets\n",
    "\n",
    "    # Calculate period-specific metrics\n",
    "    period_mae = mean_absolute_error(test_periods_true, test_periods_pred)\n",
    "    period_mse = mean_squared_error(test_periods_true, test_periods_pred)\n",
    "    period_rmse = np.sqrt(period_mse)\n",
    "    period_r2 = r2_score(test_periods_true, test_periods_pred)\n",
    "\n",
    "    return {\n",
    "        \"Frequency MSE\": mse,\n",
    "        \"Frequency RMSE\": rmse,\n",
    "        \"Frequency MAE\": mae,\n",
    "        \"Frequency R2\": r2,\n",
    "        \"Period MAE\": period_mae,\n",
    "        \"Period RMSE\": period_rmse,\n",
    "        \"Period R2\": period_r2,\n",
    "        \"Predictions\": test_preds,\n",
    "        \"True Values\": test_targets,\n",
    "        \"Period Predictions\": test_periods_pred,\n",
    "        \"True Periods\": test_periods_true,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on test set: 100%|██████████| 6/6 [00:00<00:00, 787.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency MSE: 2.5998\n",
      "Frequency RMSE: 1.6124\n",
      "Frequency MAE: 1.1652\n",
      "Frequency R2: 0.1273\n",
      "Period MAE: 4.1774\n",
      "Period RMSE: 7.1485\n",
      "Period R2: 0.1006\n",
      "Predictions: [2.5978858 3.2501724 4.708563  2.7569394 1.3529259] ... [3.7265263 3.5082579 3.182117  2.5361636 5.026518 ]\n",
      "True Values: [1.5298998  3.8621905  8.243286   0.60228866 1.2255026 ] ... [3.4471862 3.4124355 2.9316916 1.3523716 4.6582766]\n",
      "Period Predictions: [ 9.238281   7.384224   5.0970964  8.705305  17.73933  ] ... [6.4403143 6.8410025 7.542149  9.463112  4.7746773]\n",
      "True Periods: [15.687302   6.2140903  2.9114602 39.848003  19.583801 ] ... [ 6.9622    7.0331    8.186399 17.746601  5.15212 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(trained_model, test_loader, device, target_scaler)\n",
    "\n",
    "# Print the results\n",
    "for metric, value in results.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"{metric}: {value[:5]} ... {value[-5:]}\")\n",
    "    else:\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
