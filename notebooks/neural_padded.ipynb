{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import TypedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from constants import DATA_DIR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from astrofit.utils import AsteroidLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "asteroid_loader = AsteroidLoader(DATA_DIR)\n",
    "\n",
    "ASTEROIDS_FREQ_DATA_PATH = DATA_DIR / \"asteroids_freq_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ASTEROIDS_FREQ_DATA_PATH, \"r\") as f:\n",
    "    asteroids_freq_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config, asteroids_data = asteroids_freq_data[\"config\"], asteroids_freq_data[\"asteroids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 2662 asteroids (40.41% failed)\n"
     ]
    }
   ],
   "source": [
    "filtered_data = {name: data for name, data in asteroids_data.items() if not data[\"is_failed\"]}\n",
    "print(f\"Filtered {len(filtered_data)} asteroids ({100*(len(asteroids_data) - len(filtered_data)) / len(asteroids_data):.2f}% failed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsteroidData(TypedDict):\n",
    "    is_failed: bool\n",
    "    reason: str | None\n",
    "    period: float\n",
    "    processing_time: float\n",
    "    freq_features: list[list]  # 1 - 4 sequences of 50 floats (freqs) from 0 to 12\n",
    "    pow_features: list[list]  # 1 - 4 sequences of 50 floats (pows) from 0 to 1 (the same shape as freq_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = {name: AsteroidData(**data) for name, data in filtered_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2129 asteroids (79.98)\n",
      "Validation: 357 asteroids (13.41)\n",
      "Test: 176 asteroids (6.61)\n"
     ]
    }
   ],
   "source": [
    "train_keys, val_test_keys = train_test_split(list(filtered_data.keys()), test_size=0.2, random_state=884288)\n",
    "val_keys, test_keys = train_test_split(val_test_keys, test_size=0.33, random_state=884288)\n",
    "\n",
    "print(f\"Train: {len(train_keys)} asteroids ({100 * len(train_keys) / len(filtered_data):.2f})\")\n",
    "print(f\"Validation: {len(val_keys)} asteroids ({100*len(val_keys) / len(filtered_data):.2f})\")\n",
    "print(f\"Test: {len(test_keys)} asteroids ({100 * len(test_keys) / len(filtered_data):.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = (\n",
    "    {key: filtered_data[key] for key in train_keys},\n",
    "    {key: filtered_data[key] for key in val_keys},\n",
    "    {key: filtered_data[key] for key in test_keys},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data_set: dict[str, AsteroidData]) -> list[np.ndarray]:\n",
    "    freq_features = [np.array(sample[\"freq_features\"]) for sample in data_set.values()]\n",
    "    pow_features = [np.array(sample[\"pow_features\"]) for sample in data_set.values()]\n",
    "    return [np.stack([freqs, pows], axis=2) for freqs, pows in zip(freq_features, pow_features)]\n",
    "\n",
    "\n",
    "def standardize_and_pad_sequences(sequences: list[np.ndarray], max_len: int, scaler=None) -> tuple[np.ndarray, StandardScaler]:\n",
    "    # Flatten all sequences\n",
    "    flat_sequences = np.vstack([seq.reshape(-1, seq.shape[-1]) for seq in sequences])\n",
    "\n",
    "    # Fit or transform with scaler\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        flat_standardized = scaler.fit_transform(flat_sequences)\n",
    "    else:\n",
    "        flat_standardized = scaler.transform(flat_sequences)\n",
    "\n",
    "    # Reshape back to original sequence shapes\n",
    "    standardized_sequences = []\n",
    "    start = 0\n",
    "    for seq in sequences:\n",
    "        end = start + seq.shape[0] * seq.shape[1]\n",
    "        standardized_seq = flat_standardized[start:end].reshape(seq.shape)\n",
    "        standardized_sequences.append(standardized_seq)\n",
    "        start = end\n",
    "\n",
    "    # Pad standardized sequences with a special value (e.g., -1000)\n",
    "    padded = [\n",
    "        np.pad(seq, ((0, max_len - len(seq)), (0, 0), (0, 0)), mode=\"constant\", constant_values=-1000)\n",
    "        for seq in standardized_sequences\n",
    "    ]\n",
    "    return np.stack(padded), scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = extract_features(train_set)\n",
    "val_features = extract_features(val_set)\n",
    "test_features = extract_features(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max length\n",
    "max_len = max(  # equals to 4 but just in case\n",
    "    max(len(seq) for seq in train_features),\n",
    "    max(len(seq) for seq in val_features),\n",
    "    max(len(seq) for seq in test_features),\n",
    ")\n",
    "\n",
    "# Standardize and pad\n",
    "train_features_scaled, feature_scaler = standardize_and_pad_sequences(train_features, max_len)\n",
    "val_features_scaled, _ = standardize_and_pad_sequences(val_features, max_len, scaler=feature_scaler)\n",
    "test_features_scaled, _ = standardize_and_pad_sequences(test_features, max_len, scaler=feature_scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-3.2857769060746778), np.float64(29.815852864803333))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find max value in train_features_scaled but not equal to -1000\n",
    "max_value = np.max(train_features_scaled[train_features_scaled != -1000])\n",
    "min_value = np.min(train_features_scaled[train_features_scaled != -1000])\n",
    "min_value, max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_periods = np.array([sample[\"period\"] for sample in train_set.values()])\n",
    "val_periods = np.array([sample[\"period\"] for sample in val_set.values()])\n",
    "test_periods = np.array([sample[\"period\"] for sample in test_set.values()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_freqs = 24 / train_periods\n",
    "val_freqs = 24 / val_periods\n",
    "test_freqs = 24 / test_periods\n",
    "\n",
    "# Standardize target frequencies\n",
    "target_scaler = StandardScaler()\n",
    "train_freqs_scaled = target_scaler.fit_transform(train_freqs.reshape(-1, 1)).flatten()\n",
    "val_freqs_scaled = target_scaler.transform(val_freqs.reshape(-1, 1)).flatten()\n",
    "test_freqs_scaled = target_scaler.transform(test_freqs.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsteroidDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray, targets: np.ndarray):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AsteroidDataset(train_features_scaled, train_freqs_scaled)\n",
    "val_dataset = AsteroidDataset(val_features_scaled, val_freqs_scaled)\n",
    "test_dataset = AsteroidDataset(test_features_scaled, test_freqs_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsteroidPeriodPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AsteroidPeriodPredictor, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, kernel_size=(3, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_sessions, num_freq, num_features = x.shape\n",
    "\n",
    "        x = x.permute(0, 3, 1, 2)  # Change to (batch_size, channels, sessions, frequencies)\n",
    "\n",
    "        x = self.cnn(x)\n",
    "\n",
    "        x = x.view(batch_size, -1)  # Flatten: (batch_size, 128)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AsteroidPeriodPredictor(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(2, 32, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.25, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AsteroidPeriodPredictor().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 4, 50, 2]), torch.Size([32]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, targets = next(iter(train_loader))\n",
    "data.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=1000, patience=50):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.3, patience=40)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_targets = [], []\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(outputs.cpu().detach().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_r2 = r2_score(train_targets, train_preds)\n",
    "        train_mae = mean_absolute_error(train_targets, train_preds)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_targets = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, targets in val_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                outputs = model(data)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_r2 = r2_score(val_targets, val_preds)\n",
    "        val_mae = mean_absolute_error(val_targets, val_preds)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"{epoch + 1}/{num_epochs} - \", end=\"\")\n",
    "        print(f\"Train-loss: {train_loss:.4f}, Train-R2: {train_r2:.4f}, Train-MAE: {train_mae:.4f}\", end=\"\\t- \")\n",
    "        print(f\"Val-loss: {val_loss:.4f}, Val-R2: {val_r2:.4f}, Val-MAE: {val_mae:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "\n",
    "    print(f\"Best validation loss: {best_val_loss}\")\n",
    "\n",
    "    # Load the best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1000 - Train-loss: 1.0180, Train-R2: -0.0179, Train-MAE: 0.8180\t- Val-loss: 1.0877, Val-R2: 0.0086, Val-MAE: 0.8182\n",
      "2/1000 - Train-loss: 1.0062, Train-R2: -0.0066, Train-MAE: 0.8090\t- Val-loss: 1.1051, Val-R2: -0.0006, Val-MAE: 0.8216\n",
      "3/1000 - Train-loss: 1.0014, Train-R2: -0.0017, Train-MAE: 0.8100\t- Val-loss: 1.0993, Val-R2: 0.0024, Val-MAE: 0.8210\n",
      "4/1000 - Train-loss: 0.9969, Train-R2: 0.0001, Train-MAE: 0.8092\t- Val-loss: 1.1071, Val-R2: -0.0056, Val-MAE: 0.8196\n",
      "5/1000 - Train-loss: 1.0075, Train-R2: -0.0008, Train-MAE: 0.8083\t- Val-loss: 1.0997, Val-R2: -0.0018, Val-MAE: 0.8341\n",
      "6/1000 - Train-loss: 1.0038, Train-R2: -0.0030, Train-MAE: 0.8127\t- Val-loss: 1.0963, Val-R2: 0.0011, Val-MAE: 0.8219\n",
      "7/1000 - Train-loss: 1.0034, Train-R2: -0.0005, Train-MAE: 0.8120\t- Val-loss: 1.0948, Val-R2: 0.0030, Val-MAE: 0.8200\n",
      "8/1000 - Train-loss: 0.9992, Train-R2: 0.0010, Train-MAE: 0.8087\t- Val-loss: 1.0928, Val-R2: 0.0046, Val-MAE: 0.8236\n",
      "9/1000 - Train-loss: 1.0015, Train-R2: -0.0003, Train-MAE: 0.8132\t- Val-loss: 1.0988, Val-R2: 0.0013, Val-MAE: 0.8192\n",
      "10/1000 - Train-loss: 0.9955, Train-R2: 0.0010, Train-MAE: 0.8094\t- Val-loss: 1.0971, Val-R2: 0.0028, Val-MAE: 0.8209\n",
      "11/1000 - Train-loss: 0.9969, Train-R2: 0.0033, Train-MAE: 0.8103\t- Val-loss: 1.0968, Val-R2: 0.0038, Val-MAE: 0.8170\n",
      "12/1000 - Train-loss: 1.0003, Train-R2: 0.0000, Train-MAE: 0.8090\t- Val-loss: 1.0982, Val-R2: 0.0016, Val-MAE: 0.8194\n",
      "13/1000 - Train-loss: 0.9945, Train-R2: 0.0041, Train-MAE: 0.8081\t- Val-loss: 1.0962, Val-R2: 0.0048, Val-MAE: 0.8200\n",
      "14/1000 - Train-loss: 1.0019, Train-R2: -0.0003, Train-MAE: 0.8112\t- Val-loss: 1.0949, Val-R2: 0.0040, Val-MAE: 0.8219\n",
      "15/1000 - Train-loss: 0.9957, Train-R2: 0.0018, Train-MAE: 0.8106\t- Val-loss: 1.0993, Val-R2: 0.0016, Val-MAE: 0.8198\n",
      "16/1000 - Train-loss: 1.0007, Train-R2: 0.0011, Train-MAE: 0.8096\t- Val-loss: 1.0947, Val-R2: 0.0043, Val-MAE: 0.8216\n",
      "17/1000 - Train-loss: 1.0019, Train-R2: 0.0010, Train-MAE: 0.8111\t- Val-loss: 1.0953, Val-R2: 0.0040, Val-MAE: 0.8202\n",
      "18/1000 - Train-loss: 1.0018, Train-R2: 0.0017, Train-MAE: 0.8080\t- Val-loss: 1.0956, Val-R2: 0.0038, Val-MAE: 0.8217\n",
      "19/1000 - Train-loss: 0.9960, Train-R2: 0.0006, Train-MAE: 0.8112\t- Val-loss: 1.0979, Val-R2: 0.0021, Val-MAE: 0.8207\n",
      "20/1000 - Train-loss: 0.9970, Train-R2: 0.0021, Train-MAE: 0.8088\t- Val-loss: 1.0950, Val-R2: 0.0051, Val-MAE: 0.8217\n",
      "21/1000 - Train-loss: 0.9967, Train-R2: 0.0005, Train-MAE: 0.8115\t- Val-loss: 1.0922, Val-R2: 0.0062, Val-MAE: 0.8217\n",
      "22/1000 - Train-loss: 0.9992, Train-R2: 0.0035, Train-MAE: 0.8101\t- Val-loss: 1.0983, Val-R2: 0.0027, Val-MAE: 0.8212\n",
      "23/1000 - Train-loss: 1.0009, Train-R2: 0.0004, Train-MAE: 0.8107\t- Val-loss: 1.0943, Val-R2: 0.0049, Val-MAE: 0.8206\n",
      "24/1000 - Train-loss: 0.9972, Train-R2: 0.0013, Train-MAE: 0.8104\t- Val-loss: 1.0973, Val-R2: 0.0026, Val-MAE: 0.8199\n",
      "25/1000 - Train-loss: 0.9968, Train-R2: 0.0033, Train-MAE: 0.8098\t- Val-loss: 1.0992, Val-R2: 0.0022, Val-MAE: 0.8199\n",
      "26/1000 - Train-loss: 1.0012, Train-R2: 0.0016, Train-MAE: 0.8111\t- Val-loss: 1.0997, Val-R2: 0.0015, Val-MAE: 0.8205\n",
      "27/1000 - Train-loss: 0.9973, Train-R2: 0.0039, Train-MAE: 0.8083\t- Val-loss: 1.1003, Val-R2: 0.0020, Val-MAE: 0.8196\n",
      "28/1000 - Train-loss: 1.0002, Train-R2: 0.0010, Train-MAE: 0.8102\t- Val-loss: 1.0987, Val-R2: 0.0017, Val-MAE: 0.8206\n",
      "29/1000 - Train-loss: 1.0013, Train-R2: 0.0024, Train-MAE: 0.8092\t- Val-loss: 1.0987, Val-R2: 0.0019, Val-MAE: 0.8212\n",
      "30/1000 - Train-loss: 0.9985, Train-R2: 0.0010, Train-MAE: 0.8088\t- Val-loss: 1.0990, Val-R2: 0.0018, Val-MAE: 0.8217\n",
      "31/1000 - Train-loss: 0.9978, Train-R2: 0.0005, Train-MAE: 0.8095\t- Val-loss: 1.0971, Val-R2: 0.0025, Val-MAE: 0.8216\n",
      "32/1000 - Train-loss: 0.9942, Train-R2: 0.0030, Train-MAE: 0.8108\t- Val-loss: 1.0973, Val-R2: 0.0024, Val-MAE: 0.8207\n",
      "33/1000 - Train-loss: 1.0008, Train-R2: 0.0007, Train-MAE: 0.8091\t- Val-loss: 1.0975, Val-R2: 0.0016, Val-MAE: 0.8212\n",
      "34/1000 - Train-loss: 0.9971, Train-R2: 0.0022, Train-MAE: 0.8113\t- Val-loss: 1.0973, Val-R2: 0.0028, Val-MAE: 0.8224\n",
      "35/1000 - Train-loss: 1.0014, Train-R2: 0.0006, Train-MAE: 0.8111\t- Val-loss: 1.0990, Val-R2: 0.0012, Val-MAE: 0.8193\n",
      "36/1000 - Train-loss: 0.9965, Train-R2: 0.0018, Train-MAE: 0.8087\t- Val-loss: 1.0982, Val-R2: 0.0022, Val-MAE: 0.8189\n",
      "37/1000 - Train-loss: 0.9954, Train-R2: 0.0024, Train-MAE: 0.8097\t- Val-loss: 1.0978, Val-R2: 0.0023, Val-MAE: 0.8199\n",
      "38/1000 - Train-loss: 0.9936, Train-R2: 0.0022, Train-MAE: 0.8097\t- Val-loss: 1.0971, Val-R2: 0.0036, Val-MAE: 0.8204\n",
      "39/1000 - Train-loss: 0.9964, Train-R2: 0.0031, Train-MAE: 0.8081\t- Val-loss: 1.0968, Val-R2: 0.0034, Val-MAE: 0.8223\n",
      "40/1000 - Train-loss: 0.9986, Train-R2: 0.0028, Train-MAE: 0.8097\t- Val-loss: 1.0976, Val-R2: 0.0031, Val-MAE: 0.8218\n",
      "41/1000 - Train-loss: 0.9959, Train-R2: 0.0023, Train-MAE: 0.8105\t- Val-loss: 1.0982, Val-R2: 0.0019, Val-MAE: 0.8194\n",
      "42/1000 - Train-loss: 1.0017, Train-R2: 0.0028, Train-MAE: 0.8088\t- Val-loss: 1.0984, Val-R2: 0.0019, Val-MAE: 0.8197\n",
      "43/1000 - Train-loss: 0.9946, Train-R2: 0.0032, Train-MAE: 0.8085\t- Val-loss: 1.0981, Val-R2: 0.0023, Val-MAE: 0.8200\n",
      "44/1000 - Train-loss: 0.9953, Train-R2: 0.0026, Train-MAE: 0.8085\t- Val-loss: 1.0985, Val-R2: 0.0021, Val-MAE: 0.8202\n",
      "45/1000 - Train-loss: 0.9945, Train-R2: 0.0031, Train-MAE: 0.8079\t- Val-loss: 1.0988, Val-R2: 0.0023, Val-MAE: 0.8197\n",
      "46/1000 - Train-loss: 0.9933, Train-R2: 0.0031, Train-MAE: 0.8085\t- Val-loss: 1.0982, Val-R2: 0.0027, Val-MAE: 0.8204\n",
      "47/1000 - Train-loss: 0.9955, Train-R2: 0.0032, Train-MAE: 0.8085\t- Val-loss: 1.0985, Val-R2: 0.0024, Val-MAE: 0.8205\n",
      "48/1000 - Train-loss: 1.0026, Train-R2: 0.0045, Train-MAE: 0.8087\t- Val-loss: 1.0983, Val-R2: 0.0027, Val-MAE: 0.8206\n",
      "49/1000 - Train-loss: 0.9969, Train-R2: 0.0029, Train-MAE: 0.8099\t- Val-loss: 1.0977, Val-R2: 0.0030, Val-MAE: 0.8215\n",
      "50/1000 - Train-loss: 0.9933, Train-R2: 0.0039, Train-MAE: 0.8083\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8208\n",
      "51/1000 - Train-loss: 1.0011, Train-R2: 0.0029, Train-MAE: 0.8091\t- Val-loss: 1.0979, Val-R2: 0.0027, Val-MAE: 0.8207\n",
      "52/1000 - Train-loss: 1.0001, Train-R2: 0.0026, Train-MAE: 0.8094\t- Val-loss: 1.0980, Val-R2: 0.0027, Val-MAE: 0.8206\n",
      "53/1000 - Train-loss: 0.9971, Train-R2: 0.0033, Train-MAE: 0.8091\t- Val-loss: 1.0972, Val-R2: 0.0033, Val-MAE: 0.8207\n",
      "54/1000 - Train-loss: 1.0024, Train-R2: 0.0038, Train-MAE: 0.8085\t- Val-loss: 1.0972, Val-R2: 0.0032, Val-MAE: 0.8207\n",
      "55/1000 - Train-loss: 1.0001, Train-R2: 0.0032, Train-MAE: 0.8094\t- Val-loss: 1.0968, Val-R2: 0.0035, Val-MAE: 0.8210\n",
      "56/1000 - Train-loss: 0.9956, Train-R2: 0.0037, Train-MAE: 0.8086\t- Val-loss: 1.0973, Val-R2: 0.0035, Val-MAE: 0.8210\n",
      "57/1000 - Train-loss: 0.9998, Train-R2: 0.0027, Train-MAE: 0.8091\t- Val-loss: 1.0975, Val-R2: 0.0031, Val-MAE: 0.8201\n",
      "58/1000 - Train-loss: 0.9939, Train-R2: 0.0039, Train-MAE: 0.8083\t- Val-loss: 1.0972, Val-R2: 0.0035, Val-MAE: 0.8208\n",
      "59/1000 - Train-loss: 0.9956, Train-R2: 0.0041, Train-MAE: 0.8084\t- Val-loss: 1.0970, Val-R2: 0.0036, Val-MAE: 0.8214\n",
      "60/1000 - Train-loss: 0.9967, Train-R2: 0.0027, Train-MAE: 0.8102\t- Val-loss: 1.0967, Val-R2: 0.0037, Val-MAE: 0.8219\n",
      "61/1000 - Train-loss: 0.9951, Train-R2: 0.0044, Train-MAE: 0.8096\t- Val-loss: 1.0981, Val-R2: 0.0031, Val-MAE: 0.8210\n",
      "62/1000 - Train-loss: 1.0004, Train-R2: 0.0033, Train-MAE: 0.8095\t- Val-loss: 1.0982, Val-R2: 0.0029, Val-MAE: 0.8205\n",
      "63/1000 - Train-loss: 0.9945, Train-R2: 0.0054, Train-MAE: 0.8088\t- Val-loss: 1.0986, Val-R2: 0.0028, Val-MAE: 0.8214\n",
      "64/1000 - Train-loss: 0.9969, Train-R2: 0.0025, Train-MAE: 0.8094\t- Val-loss: 1.0983, Val-R2: 0.0029, Val-MAE: 0.8211\n",
      "65/1000 - Train-loss: 0.9978, Train-R2: 0.0048, Train-MAE: 0.8080\t- Val-loss: 1.0985, Val-R2: 0.0028, Val-MAE: 0.8209\n",
      "66/1000 - Train-loss: 0.9965, Train-R2: 0.0025, Train-MAE: 0.8090\t- Val-loss: 1.0983, Val-R2: 0.0027, Val-MAE: 0.8210\n",
      "67/1000 - Train-loss: 1.0012, Train-R2: 0.0037, Train-MAE: 0.8081\t- Val-loss: 1.0988, Val-R2: 0.0026, Val-MAE: 0.8202\n",
      "68/1000 - Train-loss: 0.9978, Train-R2: 0.0040, Train-MAE: 0.8084\t- Val-loss: 1.0986, Val-R2: 0.0027, Val-MAE: 0.8208\n",
      "69/1000 - Train-loss: 0.9988, Train-R2: 0.0029, Train-MAE: 0.8086\t- Val-loss: 1.0982, Val-R2: 0.0027, Val-MAE: 0.8203\n",
      "70/1000 - Train-loss: 0.9986, Train-R2: 0.0036, Train-MAE: 0.8089\t- Val-loss: 1.0977, Val-R2: 0.0033, Val-MAE: 0.8213\n",
      "71/1000 - Train-loss: 0.9921, Train-R2: 0.0038, Train-MAE: 0.8083\t- Val-loss: 1.0980, Val-R2: 0.0028, Val-MAE: 0.8209\n",
      "72/1000 - Train-loss: 1.0016, Train-R2: 0.0022, Train-MAE: 0.8095\t- Val-loss: 1.0983, Val-R2: 0.0026, Val-MAE: 0.8208\n",
      "73/1000 - Train-loss: 0.9978, Train-R2: 0.0029, Train-MAE: 0.8095\t- Val-loss: 1.0980, Val-R2: 0.0027, Val-MAE: 0.8211\n",
      "74/1000 - Train-loss: 0.9991, Train-R2: 0.0029, Train-MAE: 0.8091\t- Val-loss: 1.0985, Val-R2: 0.0024, Val-MAE: 0.8203\n",
      "75/1000 - Train-loss: 0.9985, Train-R2: 0.0033, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0028, Val-MAE: 0.8207\n",
      "76/1000 - Train-loss: 0.9945, Train-R2: 0.0035, Train-MAE: 0.8095\t- Val-loss: 1.0981, Val-R2: 0.0027, Val-MAE: 0.8211\n",
      "77/1000 - Train-loss: 0.9960, Train-R2: 0.0028, Train-MAE: 0.8095\t- Val-loss: 1.0979, Val-R2: 0.0028, Val-MAE: 0.8213\n",
      "78/1000 - Train-loss: 0.9931, Train-R2: 0.0039, Train-MAE: 0.8086\t- Val-loss: 1.0980, Val-R2: 0.0026, Val-MAE: 0.8207\n",
      "79/1000 - Train-loss: 0.9956, Train-R2: 0.0041, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8216\n",
      "80/1000 - Train-loss: 0.9967, Train-R2: 0.0034, Train-MAE: 0.8096\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8208\n",
      "81/1000 - Train-loss: 0.9967, Train-R2: 0.0040, Train-MAE: 0.8087\t- Val-loss: 1.0976, Val-R2: 0.0031, Val-MAE: 0.8216\n",
      "82/1000 - Train-loss: 0.9946, Train-R2: 0.0030, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0029, Val-MAE: 0.8209\n",
      "83/1000 - Train-loss: 0.9928, Train-R2: 0.0041, Train-MAE: 0.8077\t- Val-loss: 1.0978, Val-R2: 0.0029, Val-MAE: 0.8207\n",
      "84/1000 - Train-loss: 1.0057, Train-R2: 0.0036, Train-MAE: 0.8084\t- Val-loss: 1.0979, Val-R2: 0.0030, Val-MAE: 0.8211\n",
      "85/1000 - Train-loss: 0.9941, Train-R2: 0.0032, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0029, Val-MAE: 0.8208\n",
      "86/1000 - Train-loss: 0.9988, Train-R2: 0.0022, Train-MAE: 0.8095\t- Val-loss: 1.0978, Val-R2: 0.0029, Val-MAE: 0.8211\n",
      "87/1000 - Train-loss: 0.9945, Train-R2: 0.0050, Train-MAE: 0.8090\t- Val-loss: 1.0980, Val-R2: 0.0029, Val-MAE: 0.8211\n",
      "88/1000 - Train-loss: 0.9968, Train-R2: 0.0040, Train-MAE: 0.8087\t- Val-loss: 1.0980, Val-R2: 0.0030, Val-MAE: 0.8210\n",
      "89/1000 - Train-loss: 0.9963, Train-R2: 0.0027, Train-MAE: 0.8095\t- Val-loss: 1.0980, Val-R2: 0.0029, Val-MAE: 0.8213\n",
      "90/1000 - Train-loss: 0.9936, Train-R2: 0.0033, Train-MAE: 0.8095\t- Val-loss: 1.0981, Val-R2: 0.0029, Val-MAE: 0.8216\n",
      "91/1000 - Train-loss: 1.0051, Train-R2: 0.0034, Train-MAE: 0.8087\t- Val-loss: 1.0980, Val-R2: 0.0028, Val-MAE: 0.8209\n",
      "92/1000 - Train-loss: 0.9991, Train-R2: 0.0049, Train-MAE: 0.8085\t- Val-loss: 1.0980, Val-R2: 0.0029, Val-MAE: 0.8210\n",
      "93/1000 - Train-loss: 1.0009, Train-R2: 0.0029, Train-MAE: 0.8094\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8211\n",
      "94/1000 - Train-loss: 0.9988, Train-R2: 0.0038, Train-MAE: 0.8088\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8207\n",
      "95/1000 - Train-loss: 1.0003, Train-R2: 0.0033, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "96/1000 - Train-loss: 0.9959, Train-R2: 0.0037, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8209\n",
      "97/1000 - Train-loss: 0.9934, Train-R2: 0.0041, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8212\n",
      "98/1000 - Train-loss: 0.9945, Train-R2: 0.0035, Train-MAE: 0.8092\t- Val-loss: 1.0977, Val-R2: 0.0030, Val-MAE: 0.8212\n",
      "99/1000 - Train-loss: 0.9966, Train-R2: 0.0024, Train-MAE: 0.8089\t- Val-loss: 1.0977, Val-R2: 0.0030, Val-MAE: 0.8214\n",
      "100/1000 - Train-loss: 0.9996, Train-R2: 0.0034, Train-MAE: 0.8093\t- Val-loss: 1.0977, Val-R2: 0.0030, Val-MAE: 0.8212\n",
      "101/1000 - Train-loss: 0.9944, Train-R2: 0.0030, Train-MAE: 0.8094\t- Val-loss: 1.0978, Val-R2: 0.0029, Val-MAE: 0.8212\n",
      "102/1000 - Train-loss: 0.9991, Train-R2: 0.0031, Train-MAE: 0.8091\t- Val-loss: 1.0979, Val-R2: 0.0028, Val-MAE: 0.8207\n",
      "103/1000 - Train-loss: 0.9952, Train-R2: 0.0030, Train-MAE: 0.8089\t- Val-loss: 1.0981, Val-R2: 0.0027, Val-MAE: 0.8207\n",
      "104/1000 - Train-loss: 1.0024, Train-R2: 0.0034, Train-MAE: 0.8086\t- Val-loss: 1.0981, Val-R2: 0.0028, Val-MAE: 0.8210\n",
      "105/1000 - Train-loss: 0.9950, Train-R2: 0.0026, Train-MAE: 0.8089\t- Val-loss: 1.0981, Val-R2: 0.0027, Val-MAE: 0.8210\n",
      "106/1000 - Train-loss: 0.9969, Train-R2: 0.0038, Train-MAE: 0.8090\t- Val-loss: 1.0980, Val-R2: 0.0027, Val-MAE: 0.8208\n",
      "107/1000 - Train-loss: 0.9967, Train-R2: 0.0046, Train-MAE: 0.8089\t- Val-loss: 1.0980, Val-R2: 0.0028, Val-MAE: 0.8207\n",
      "108/1000 - Train-loss: 0.9952, Train-R2: 0.0035, Train-MAE: 0.8090\t- Val-loss: 1.0980, Val-R2: 0.0028, Val-MAE: 0.8206\n",
      "109/1000 - Train-loss: 0.9941, Train-R2: 0.0026, Train-MAE: 0.8095\t- Val-loss: 1.0979, Val-R2: 0.0028, Val-MAE: 0.8208\n",
      "110/1000 - Train-loss: 0.9937, Train-R2: 0.0039, Train-MAE: 0.8086\t- Val-loss: 1.0979, Val-R2: 0.0027, Val-MAE: 0.8205\n",
      "111/1000 - Train-loss: 0.9942, Train-R2: 0.0024, Train-MAE: 0.8093\t- Val-loss: 1.0978, Val-R2: 0.0028, Val-MAE: 0.8207\n",
      "112/1000 - Train-loss: 0.9976, Train-R2: 0.0037, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0029, Val-MAE: 0.8207\n",
      "113/1000 - Train-loss: 0.9985, Train-R2: 0.0048, Train-MAE: 0.8081\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8206\n",
      "114/1000 - Train-loss: 0.9970, Train-R2: 0.0041, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0029, Val-MAE: 0.8208\n",
      "115/1000 - Train-loss: 1.0015, Train-R2: 0.0047, Train-MAE: 0.8082\t- Val-loss: 1.0979, Val-R2: 0.0030, Val-MAE: 0.8211\n",
      "116/1000 - Train-loss: 0.9938, Train-R2: 0.0028, Train-MAE: 0.8093\t- Val-loss: 1.0977, Val-R2: 0.0030, Val-MAE: 0.8212\n",
      "117/1000 - Train-loss: 0.9943, Train-R2: 0.0029, Train-MAE: 0.8093\t- Val-loss: 1.0978, Val-R2: 0.0029, Val-MAE: 0.8208\n",
      "118/1000 - Train-loss: 0.9984, Train-R2: 0.0021, Train-MAE: 0.8099\t- Val-loss: 1.0979, Val-R2: 0.0028, Val-MAE: 0.8210\n",
      "119/1000 - Train-loss: 0.9928, Train-R2: 0.0039, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0028, Val-MAE: 0.8209\n",
      "120/1000 - Train-loss: 0.9922, Train-R2: 0.0044, Train-MAE: 0.8090\t- Val-loss: 1.0977, Val-R2: 0.0030, Val-MAE: 0.8211\n",
      "121/1000 - Train-loss: 0.9978, Train-R2: 0.0037, Train-MAE: 0.8094\t- Val-loss: 1.0977, Val-R2: 0.0030, Val-MAE: 0.8214\n",
      "122/1000 - Train-loss: 0.9987, Train-R2: 0.0051, Train-MAE: 0.8093\t- Val-loss: 1.0977, Val-R2: 0.0030, Val-MAE: 0.8215\n",
      "123/1000 - Train-loss: 0.9950, Train-R2: 0.0029, Train-MAE: 0.8101\t- Val-loss: 1.0978, Val-R2: 0.0029, Val-MAE: 0.8214\n",
      "124/1000 - Train-loss: 0.9952, Train-R2: 0.0036, Train-MAE: 0.8094\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8213\n",
      "125/1000 - Train-loss: 0.9993, Train-R2: 0.0040, Train-MAE: 0.8091\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8213\n",
      "126/1000 - Train-loss: 0.9975, Train-R2: 0.0036, Train-MAE: 0.8090\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8212\n",
      "127/1000 - Train-loss: 0.9958, Train-R2: 0.0039, Train-MAE: 0.8091\t- Val-loss: 1.0979, Val-R2: 0.0030, Val-MAE: 0.8214\n",
      "128/1000 - Train-loss: 0.9982, Train-R2: 0.0042, Train-MAE: 0.8089\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8211\n",
      "129/1000 - Train-loss: 0.9997, Train-R2: 0.0039, Train-MAE: 0.8095\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8212\n",
      "130/1000 - Train-loss: 1.0016, Train-R2: 0.0024, Train-MAE: 0.8095\t- Val-loss: 1.0979, Val-R2: 0.0030, Val-MAE: 0.8214\n",
      "131/1000 - Train-loss: 0.9930, Train-R2: 0.0032, Train-MAE: 0.8092\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8212\n",
      "132/1000 - Train-loss: 1.0011, Train-R2: 0.0039, Train-MAE: 0.8090\t- Val-loss: 1.0979, Val-R2: 0.0028, Val-MAE: 0.8209\n",
      "133/1000 - Train-loss: 0.9938, Train-R2: 0.0042, Train-MAE: 0.8092\t- Val-loss: 1.0979, Val-R2: 0.0030, Val-MAE: 0.8212\n",
      "134/1000 - Train-loss: 0.9984, Train-R2: 0.0048, Train-MAE: 0.8086\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8211\n",
      "135/1000 - Train-loss: 0.9938, Train-R2: 0.0051, Train-MAE: 0.8082\t- Val-loss: 1.0979, Val-R2: 0.0030, Val-MAE: 0.8213\n",
      "136/1000 - Train-loss: 0.9930, Train-R2: 0.0042, Train-MAE: 0.8088\t- Val-loss: 1.0979, Val-R2: 0.0030, Val-MAE: 0.8212\n",
      "137/1000 - Train-loss: 0.9955, Train-R2: 0.0029, Train-MAE: 0.8100\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8210\n",
      "138/1000 - Train-loss: 1.0007, Train-R2: 0.0026, Train-MAE: 0.8093\t- Val-loss: 1.0979, Val-R2: 0.0030, Val-MAE: 0.8211\n",
      "139/1000 - Train-loss: 0.9949, Train-R2: 0.0040, Train-MAE: 0.8091\t- Val-loss: 1.0979, Val-R2: 0.0030, Val-MAE: 0.8209\n",
      "140/1000 - Train-loss: 1.0002, Train-R2: 0.0027, Train-MAE: 0.8094\t- Val-loss: 1.0979, Val-R2: 0.0030, Val-MAE: 0.8212\n",
      "141/1000 - Train-loss: 0.9957, Train-R2: 0.0032, Train-MAE: 0.8090\t- Val-loss: 1.0979, Val-R2: 0.0030, Val-MAE: 0.8210\n",
      "142/1000 - Train-loss: 0.9969, Train-R2: 0.0043, Train-MAE: 0.8085\t- Val-loss: 1.0979, Val-R2: 0.0031, Val-MAE: 0.8213\n",
      "143/1000 - Train-loss: 0.9951, Train-R2: 0.0040, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8209\n",
      "144/1000 - Train-loss: 0.9964, Train-R2: 0.0022, Train-MAE: 0.8096\t- Val-loss: 1.0979, Val-R2: 0.0030, Val-MAE: 0.8210\n",
      "145/1000 - Train-loss: 0.9949, Train-R2: 0.0024, Train-MAE: 0.8095\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "146/1000 - Train-loss: 0.9933, Train-R2: 0.0043, Train-MAE: 0.8086\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8208\n",
      "147/1000 - Train-loss: 0.9969, Train-R2: 0.0031, Train-MAE: 0.8085\t- Val-loss: 1.0979, Val-R2: 0.0030, Val-MAE: 0.8209\n",
      "148/1000 - Train-loss: 0.9946, Train-R2: 0.0044, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8209\n",
      "149/1000 - Train-loss: 0.9955, Train-R2: 0.0048, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8209\n",
      "150/1000 - Train-loss: 0.9944, Train-R2: 0.0033, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8209\n",
      "151/1000 - Train-loss: 0.9961, Train-R2: 0.0043, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8209\n",
      "152/1000 - Train-loss: 0.9954, Train-R2: 0.0036, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8209\n",
      "153/1000 - Train-loss: 0.9960, Train-R2: 0.0038, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8210\n",
      "154/1000 - Train-loss: 0.9976, Train-R2: 0.0036, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8211\n",
      "155/1000 - Train-loss: 0.9982, Train-R2: 0.0038, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "156/1000 - Train-loss: 0.9934, Train-R2: 0.0031, Train-MAE: 0.8091\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "157/1000 - Train-loss: 0.9938, Train-R2: 0.0035, Train-MAE: 0.8093\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "158/1000 - Train-loss: 0.9985, Train-R2: 0.0015, Train-MAE: 0.8092\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8209\n",
      "159/1000 - Train-loss: 0.9999, Train-R2: 0.0047, Train-MAE: 0.8082\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8210\n",
      "160/1000 - Train-loss: 1.0053, Train-R2: 0.0022, Train-MAE: 0.8095\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8205\n",
      "161/1000 - Train-loss: 0.9955, Train-R2: 0.0042, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "162/1000 - Train-loss: 0.9971, Train-R2: 0.0031, Train-MAE: 0.8087\t- Val-loss: 1.0979, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "163/1000 - Train-loss: 0.9984, Train-R2: 0.0039, Train-MAE: 0.8085\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8204\n",
      "164/1000 - Train-loss: 0.9991, Train-R2: 0.0031, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "165/1000 - Train-loss: 0.9973, Train-R2: 0.0040, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8209\n",
      "166/1000 - Train-loss: 0.9962, Train-R2: 0.0030, Train-MAE: 0.8091\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8205\n",
      "167/1000 - Train-loss: 1.0008, Train-R2: 0.0036, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "168/1000 - Train-loss: 1.0013, Train-R2: 0.0031, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0029, Val-MAE: 0.8206\n",
      "169/1000 - Train-loss: 0.9927, Train-R2: 0.0042, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0029, Val-MAE: 0.8206\n",
      "170/1000 - Train-loss: 0.9993, Train-R2: 0.0030, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "171/1000 - Train-loss: 0.9953, Train-R2: 0.0023, Train-MAE: 0.8093\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "172/1000 - Train-loss: 0.9954, Train-R2: 0.0040, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "173/1000 - Train-loss: 0.9976, Train-R2: 0.0042, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0029, Val-MAE: 0.8206\n",
      "174/1000 - Train-loss: 0.9953, Train-R2: 0.0033, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "175/1000 - Train-loss: 0.9943, Train-R2: 0.0037, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0029, Val-MAE: 0.8205\n",
      "176/1000 - Train-loss: 0.9992, Train-R2: 0.0035, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "177/1000 - Train-loss: 0.9990, Train-R2: 0.0032, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "178/1000 - Train-loss: 0.9975, Train-R2: 0.0045, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8209\n",
      "179/1000 - Train-loss: 0.9974, Train-R2: 0.0018, Train-MAE: 0.8093\t- Val-loss: 1.0978, Val-R2: 0.0029, Val-MAE: 0.8205\n",
      "180/1000 - Train-loss: 0.9931, Train-R2: 0.0041, Train-MAE: 0.8083\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "181/1000 - Train-loss: 0.9966, Train-R2: 0.0036, Train-MAE: 0.8089\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8204\n",
      "182/1000 - Train-loss: 0.9957, Train-R2: 0.0029, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "183/1000 - Train-loss: 0.9957, Train-R2: 0.0043, Train-MAE: 0.8080\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "184/1000 - Train-loss: 0.9956, Train-R2: 0.0050, Train-MAE: 0.8080\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "185/1000 - Train-loss: 1.0020, Train-R2: 0.0038, Train-MAE: 0.8091\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "186/1000 - Train-loss: 0.9932, Train-R2: 0.0050, Train-MAE: 0.8081\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "187/1000 - Train-loss: 0.9942, Train-R2: 0.0043, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "188/1000 - Train-loss: 1.0012, Train-R2: 0.0037, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "189/1000 - Train-loss: 1.0001, Train-R2: 0.0028, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "190/1000 - Train-loss: 0.9964, Train-R2: 0.0042, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "191/1000 - Train-loss: 0.9950, Train-R2: 0.0047, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "192/1000 - Train-loss: 0.9981, Train-R2: 0.0045, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "193/1000 - Train-loss: 0.9928, Train-R2: 0.0048, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "194/1000 - Train-loss: 0.9943, Train-R2: 0.0047, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "195/1000 - Train-loss: 0.9982, Train-R2: 0.0034, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "196/1000 - Train-loss: 0.9939, Train-R2: 0.0046, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "197/1000 - Train-loss: 0.9951, Train-R2: 0.0036, Train-MAE: 0.8091\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "198/1000 - Train-loss: 0.9927, Train-R2: 0.0038, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "199/1000 - Train-loss: 0.9957, Train-R2: 0.0036, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "200/1000 - Train-loss: 0.9929, Train-R2: 0.0042, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "201/1000 - Train-loss: 0.9975, Train-R2: 0.0042, Train-MAE: 0.8083\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "202/1000 - Train-loss: 1.0004, Train-R2: 0.0017, Train-MAE: 0.8094\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "203/1000 - Train-loss: 0.9971, Train-R2: 0.0040, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "204/1000 - Train-loss: 1.0010, Train-R2: 0.0021, Train-MAE: 0.8094\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "205/1000 - Train-loss: 0.9938, Train-R2: 0.0043, Train-MAE: 0.8083\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "206/1000 - Train-loss: 0.9965, Train-R2: 0.0039, Train-MAE: 0.8083\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "207/1000 - Train-loss: 0.9936, Train-R2: 0.0030, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "208/1000 - Train-loss: 0.9999, Train-R2: 0.0037, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "209/1000 - Train-loss: 0.9957, Train-R2: 0.0033, Train-MAE: 0.8092\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "210/1000 - Train-loss: 1.0000, Train-R2: 0.0037, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "211/1000 - Train-loss: 0.9989, Train-R2: 0.0038, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "212/1000 - Train-loss: 1.0031, Train-R2: 0.0043, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "213/1000 - Train-loss: 0.9949, Train-R2: 0.0047, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "214/1000 - Train-loss: 1.0019, Train-R2: 0.0048, Train-MAE: 0.8080\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "215/1000 - Train-loss: 0.9919, Train-R2: 0.0044, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "216/1000 - Train-loss: 0.9978, Train-R2: 0.0031, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "217/1000 - Train-loss: 0.9928, Train-R2: 0.0045, Train-MAE: 0.8082\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "218/1000 - Train-loss: 0.9943, Train-R2: 0.0032, Train-MAE: 0.8093\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "219/1000 - Train-loss: 0.9967, Train-R2: 0.0044, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "220/1000 - Train-loss: 0.9975, Train-R2: 0.0040, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "221/1000 - Train-loss: 0.9965, Train-R2: 0.0035, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "222/1000 - Train-loss: 0.9950, Train-R2: 0.0034, Train-MAE: 0.8092\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "223/1000 - Train-loss: 0.9991, Train-R2: 0.0037, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "224/1000 - Train-loss: 0.9946, Train-R2: 0.0038, Train-MAE: 0.8086\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8205\n",
      "225/1000 - Train-loss: 0.9945, Train-R2: 0.0030, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8210\n",
      "226/1000 - Train-loss: 0.9958, Train-R2: 0.0037, Train-MAE: 0.8087\t- Val-loss: 1.0979, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "227/1000 - Train-loss: 0.9913, Train-R2: 0.0054, Train-MAE: 0.8082\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "228/1000 - Train-loss: 0.9967, Train-R2: 0.0021, Train-MAE: 0.8095\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "229/1000 - Train-loss: 1.0050, Train-R2: 0.0043, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "230/1000 - Train-loss: 0.9970, Train-R2: 0.0047, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "231/1000 - Train-loss: 0.9913, Train-R2: 0.0052, Train-MAE: 0.8077\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8204\n",
      "232/1000 - Train-loss: 0.9933, Train-R2: 0.0043, Train-MAE: 0.8080\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "233/1000 - Train-loss: 0.9949, Train-R2: 0.0025, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "234/1000 - Train-loss: 0.9964, Train-R2: 0.0033, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "235/1000 - Train-loss: 0.9967, Train-R2: 0.0032, Train-MAE: 0.8094\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "236/1000 - Train-loss: 0.9993, Train-R2: 0.0041, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "237/1000 - Train-loss: 0.9971, Train-R2: 0.0029, Train-MAE: 0.8089\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8204\n",
      "238/1000 - Train-loss: 0.9935, Train-R2: 0.0051, Train-MAE: 0.8083\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "239/1000 - Train-loss: 0.9943, Train-R2: 0.0035, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "240/1000 - Train-loss: 0.9965, Train-R2: 0.0047, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "241/1000 - Train-loss: 0.9937, Train-R2: 0.0042, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "242/1000 - Train-loss: 0.9941, Train-R2: 0.0037, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "243/1000 - Train-loss: 0.9945, Train-R2: 0.0024, Train-MAE: 0.8092\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "244/1000 - Train-loss: 0.9976, Train-R2: 0.0036, Train-MAE: 0.8089\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8204\n",
      "245/1000 - Train-loss: 1.0061, Train-R2: 0.0032, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "246/1000 - Train-loss: 0.9991, Train-R2: 0.0034, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "247/1000 - Train-loss: 0.9952, Train-R2: 0.0044, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "248/1000 - Train-loss: 0.9969, Train-R2: 0.0042, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "249/1000 - Train-loss: 1.0018, Train-R2: 0.0046, Train-MAE: 0.8083\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "250/1000 - Train-loss: 0.9940, Train-R2: 0.0037, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "251/1000 - Train-loss: 0.9957, Train-R2: 0.0043, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "252/1000 - Train-loss: 0.9990, Train-R2: 0.0030, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "253/1000 - Train-loss: 0.9946, Train-R2: 0.0028, Train-MAE: 0.8092\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "254/1000 - Train-loss: 0.9965, Train-R2: 0.0044, Train-MAE: 0.8081\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "255/1000 - Train-loss: 0.9989, Train-R2: 0.0033, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "256/1000 - Train-loss: 0.9970, Train-R2: 0.0032, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "257/1000 - Train-loss: 0.9958, Train-R2: 0.0043, Train-MAE: 0.8082\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "258/1000 - Train-loss: 0.9973, Train-R2: 0.0037, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "259/1000 - Train-loss: 0.9963, Train-R2: 0.0028, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "260/1000 - Train-loss: 0.9963, Train-R2: 0.0027, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "261/1000 - Train-loss: 0.9962, Train-R2: 0.0035, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "262/1000 - Train-loss: 0.9940, Train-R2: 0.0038, Train-MAE: 0.8082\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "263/1000 - Train-loss: 0.9943, Train-R2: 0.0036, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "264/1000 - Train-loss: 0.9949, Train-R2: 0.0046, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "265/1000 - Train-loss: 0.9936, Train-R2: 0.0042, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "266/1000 - Train-loss: 0.9950, Train-R2: 0.0036, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "267/1000 - Train-loss: 0.9989, Train-R2: 0.0042, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "268/1000 - Train-loss: 1.0044, Train-R2: 0.0027, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "269/1000 - Train-loss: 0.9987, Train-R2: 0.0024, Train-MAE: 0.8092\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "270/1000 - Train-loss: 0.9999, Train-R2: 0.0047, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "271/1000 - Train-loss: 0.9961, Train-R2: 0.0043, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "272/1000 - Train-loss: 1.0021, Train-R2: 0.0045, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "273/1000 - Train-loss: 0.9960, Train-R2: 0.0036, Train-MAE: 0.8091\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "274/1000 - Train-loss: 0.9955, Train-R2: 0.0050, Train-MAE: 0.8083\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "275/1000 - Train-loss: 0.9973, Train-R2: 0.0036, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "276/1000 - Train-loss: 0.9984, Train-R2: 0.0019, Train-MAE: 0.8097\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "277/1000 - Train-loss: 0.9960, Train-R2: 0.0030, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "278/1000 - Train-loss: 0.9916, Train-R2: 0.0042, Train-MAE: 0.8080\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "279/1000 - Train-loss: 0.9994, Train-R2: 0.0039, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "280/1000 - Train-loss: 0.9942, Train-R2: 0.0038, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "281/1000 - Train-loss: 0.9954, Train-R2: 0.0050, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "282/1000 - Train-loss: 0.9965, Train-R2: 0.0044, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "283/1000 - Train-loss: 0.9938, Train-R2: 0.0048, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "284/1000 - Train-loss: 0.9923, Train-R2: 0.0042, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8210\n",
      "285/1000 - Train-loss: 0.9958, Train-R2: 0.0047, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "286/1000 - Train-loss: 0.9959, Train-R2: 0.0046, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "287/1000 - Train-loss: 0.9993, Train-R2: 0.0042, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "288/1000 - Train-loss: 0.9978, Train-R2: 0.0044, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "289/1000 - Train-loss: 0.9981, Train-R2: 0.0034, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8210\n",
      "290/1000 - Train-loss: 0.9946, Train-R2: 0.0033, Train-MAE: 0.8092\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "291/1000 - Train-loss: 0.9952, Train-R2: 0.0036, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "292/1000 - Train-loss: 0.9985, Train-R2: 0.0028, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "293/1000 - Train-loss: 0.9966, Train-R2: 0.0025, Train-MAE: 0.8093\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "294/1000 - Train-loss: 1.0000, Train-R2: 0.0036, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "295/1000 - Train-loss: 0.9942, Train-R2: 0.0041, Train-MAE: 0.8082\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "296/1000 - Train-loss: 0.9937, Train-R2: 0.0045, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "297/1000 - Train-loss: 0.9952, Train-R2: 0.0038, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "298/1000 - Train-loss: 0.9958, Train-R2: 0.0041, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "299/1000 - Train-loss: 0.9949, Train-R2: 0.0050, Train-MAE: 0.8083\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "300/1000 - Train-loss: 0.9964, Train-R2: 0.0039, Train-MAE: 0.8083\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "301/1000 - Train-loss: 0.9957, Train-R2: 0.0034, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "302/1000 - Train-loss: 0.9939, Train-R2: 0.0034, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "303/1000 - Train-loss: 0.9960, Train-R2: 0.0041, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "304/1000 - Train-loss: 0.9927, Train-R2: 0.0047, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "305/1000 - Train-loss: 0.9994, Train-R2: 0.0040, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "306/1000 - Train-loss: 0.9977, Train-R2: 0.0034, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "307/1000 - Train-loss: 0.9962, Train-R2: 0.0044, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "308/1000 - Train-loss: 0.9935, Train-R2: 0.0032, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "309/1000 - Train-loss: 0.9960, Train-R2: 0.0033, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "310/1000 - Train-loss: 0.9947, Train-R2: 0.0038, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "311/1000 - Train-loss: 0.9953, Train-R2: 0.0033, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "312/1000 - Train-loss: 0.9948, Train-R2: 0.0036, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "313/1000 - Train-loss: 0.9966, Train-R2: 0.0033, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "314/1000 - Train-loss: 0.9939, Train-R2: 0.0044, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "315/1000 - Train-loss: 0.9935, Train-R2: 0.0034, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "316/1000 - Train-loss: 0.9947, Train-R2: 0.0030, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "317/1000 - Train-loss: 0.9936, Train-R2: 0.0041, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "318/1000 - Train-loss: 1.0050, Train-R2: 0.0036, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "319/1000 - Train-loss: 0.9935, Train-R2: 0.0033, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "320/1000 - Train-loss: 0.9947, Train-R2: 0.0028, Train-MAE: 0.8092\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "321/1000 - Train-loss: 0.9970, Train-R2: 0.0029, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "322/1000 - Train-loss: 0.9971, Train-R2: 0.0034, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "323/1000 - Train-loss: 0.9948, Train-R2: 0.0023, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "324/1000 - Train-loss: 0.9937, Train-R2: 0.0038, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "325/1000 - Train-loss: 0.9966, Train-R2: 0.0037, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "326/1000 - Train-loss: 0.9929, Train-R2: 0.0041, Train-MAE: 0.8083\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8207\n",
      "327/1000 - Train-loss: 0.9983, Train-R2: 0.0033, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "328/1000 - Train-loss: 0.9975, Train-R2: 0.0030, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "329/1000 - Train-loss: 0.9963, Train-R2: 0.0040, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "330/1000 - Train-loss: 1.0005, Train-R2: 0.0031, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "331/1000 - Train-loss: 0.9926, Train-R2: 0.0041, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "332/1000 - Train-loss: 0.9933, Train-R2: 0.0043, Train-MAE: 0.8083\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "333/1000 - Train-loss: 0.9950, Train-R2: 0.0042, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "334/1000 - Train-loss: 0.9934, Train-R2: 0.0026, Train-MAE: 0.8091\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "335/1000 - Train-loss: 0.9930, Train-R2: 0.0041, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "336/1000 - Train-loss: 0.9943, Train-R2: 0.0044, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "337/1000 - Train-loss: 0.9956, Train-R2: 0.0038, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "338/1000 - Train-loss: 0.9966, Train-R2: 0.0039, Train-MAE: 0.8083\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "339/1000 - Train-loss: 0.9999, Train-R2: 0.0033, Train-MAE: 0.8091\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "340/1000 - Train-loss: 0.9926, Train-R2: 0.0040, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "341/1000 - Train-loss: 0.9955, Train-R2: 0.0038, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "342/1000 - Train-loss: 0.9944, Train-R2: 0.0036, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "343/1000 - Train-loss: 0.9967, Train-R2: 0.0035, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "344/1000 - Train-loss: 0.9930, Train-R2: 0.0041, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8210\n",
      "345/1000 - Train-loss: 0.9964, Train-R2: 0.0037, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "346/1000 - Train-loss: 0.9987, Train-R2: 0.0035, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "347/1000 - Train-loss: 0.9973, Train-R2: 0.0055, Train-MAE: 0.8080\t- Val-loss: 1.0978, Val-R2: 0.0029, Val-MAE: 0.8205\n",
      "348/1000 - Train-loss: 0.9977, Train-R2: 0.0032, Train-MAE: 0.8088\t- Val-loss: 1.0979, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "349/1000 - Train-loss: 0.9995, Train-R2: 0.0040, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "350/1000 - Train-loss: 1.0012, Train-R2: 0.0040, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "351/1000 - Train-loss: 1.0012, Train-R2: 0.0033, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "352/1000 - Train-loss: 0.9956, Train-R2: 0.0045, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "353/1000 - Train-loss: 0.9939, Train-R2: 0.0052, Train-MAE: 0.8079\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "354/1000 - Train-loss: 0.9927, Train-R2: 0.0050, Train-MAE: 0.8081\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "355/1000 - Train-loss: 1.0022, Train-R2: 0.0039, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "356/1000 - Train-loss: 0.9990, Train-R2: 0.0038, Train-MAE: 0.8081\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "357/1000 - Train-loss: 0.9976, Train-R2: 0.0027, Train-MAE: 0.8091\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "358/1000 - Train-loss: 0.9978, Train-R2: 0.0034, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "359/1000 - Train-loss: 0.9946, Train-R2: 0.0042, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "360/1000 - Train-loss: 0.9936, Train-R2: 0.0036, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "361/1000 - Train-loss: 0.9927, Train-R2: 0.0048, Train-MAE: 0.8082\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "362/1000 - Train-loss: 1.0013, Train-R2: 0.0027, Train-MAE: 0.8092\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "363/1000 - Train-loss: 0.9977, Train-R2: 0.0036, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "364/1000 - Train-loss: 1.0026, Train-R2: 0.0035, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "365/1000 - Train-loss: 0.9919, Train-R2: 0.0048, Train-MAE: 0.8082\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "366/1000 - Train-loss: 0.9950, Train-R2: 0.0043, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "367/1000 - Train-loss: 0.9947, Train-R2: 0.0039, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "368/1000 - Train-loss: 0.9975, Train-R2: 0.0017, Train-MAE: 0.8093\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "369/1000 - Train-loss: 0.9992, Train-R2: 0.0041, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "370/1000 - Train-loss: 0.9923, Train-R2: 0.0048, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "371/1000 - Train-loss: 0.9987, Train-R2: 0.0025, Train-MAE: 0.8095\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "372/1000 - Train-loss: 0.9925, Train-R2: 0.0049, Train-MAE: 0.8077\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "373/1000 - Train-loss: 0.9939, Train-R2: 0.0046, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "374/1000 - Train-loss: 0.9982, Train-R2: 0.0034, Train-MAE: 0.8091\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "375/1000 - Train-loss: 0.9947, Train-R2: 0.0035, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "376/1000 - Train-loss: 0.9989, Train-R2: 0.0015, Train-MAE: 0.8093\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "377/1000 - Train-loss: 0.9968, Train-R2: 0.0047, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "378/1000 - Train-loss: 0.9950, Train-R2: 0.0022, Train-MAE: 0.8095\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "379/1000 - Train-loss: 0.9972, Train-R2: 0.0044, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "380/1000 - Train-loss: 0.9942, Train-R2: 0.0036, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "381/1000 - Train-loss: 0.9956, Train-R2: 0.0037, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "382/1000 - Train-loss: 0.9956, Train-R2: 0.0022, Train-MAE: 0.8095\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "383/1000 - Train-loss: 0.9968, Train-R2: 0.0048, Train-MAE: 0.8080\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "384/1000 - Train-loss: 0.9954, Train-R2: 0.0040, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "385/1000 - Train-loss: 0.9938, Train-R2: 0.0035, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "386/1000 - Train-loss: 0.9991, Train-R2: 0.0039, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8207\n",
      "387/1000 - Train-loss: 0.9959, Train-R2: 0.0043, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "388/1000 - Train-loss: 0.9917, Train-R2: 0.0046, Train-MAE: 0.8079\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "389/1000 - Train-loss: 0.9945, Train-R2: 0.0043, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "390/1000 - Train-loss: 0.9958, Train-R2: 0.0039, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8211\n",
      "391/1000 - Train-loss: 0.9937, Train-R2: 0.0041, Train-MAE: 0.8082\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8204\n",
      "392/1000 - Train-loss: 0.9933, Train-R2: 0.0044, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "393/1000 - Train-loss: 0.9955, Train-R2: 0.0040, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "394/1000 - Train-loss: 0.9965, Train-R2: 0.0034, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "395/1000 - Train-loss: 0.9986, Train-R2: 0.0028, Train-MAE: 0.8091\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "396/1000 - Train-loss: 0.9979, Train-R2: 0.0033, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "397/1000 - Train-loss: 0.9975, Train-R2: 0.0043, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "398/1000 - Train-loss: 0.9955, Train-R2: 0.0038, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "399/1000 - Train-loss: 0.9976, Train-R2: 0.0033, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "400/1000 - Train-loss: 1.0003, Train-R2: 0.0032, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "401/1000 - Train-loss: 0.9973, Train-R2: 0.0043, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "402/1000 - Train-loss: 0.9962, Train-R2: 0.0026, Train-MAE: 0.8091\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8210\n",
      "403/1000 - Train-loss: 0.9975, Train-R2: 0.0040, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "404/1000 - Train-loss: 0.9957, Train-R2: 0.0045, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8211\n",
      "405/1000 - Train-loss: 0.9986, Train-R2: 0.0035, Train-MAE: 0.8091\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8210\n",
      "406/1000 - Train-loss: 0.9985, Train-R2: 0.0034, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "407/1000 - Train-loss: 0.9926, Train-R2: 0.0049, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "408/1000 - Train-loss: 0.9963, Train-R2: 0.0041, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "409/1000 - Train-loss: 0.9949, Train-R2: 0.0036, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "410/1000 - Train-loss: 0.9947, Train-R2: 0.0052, Train-MAE: 0.8082\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "411/1000 - Train-loss: 1.0002, Train-R2: 0.0038, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "412/1000 - Train-loss: 1.0027, Train-R2: 0.0022, Train-MAE: 0.8094\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "413/1000 - Train-loss: 0.9960, Train-R2: 0.0035, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "414/1000 - Train-loss: 0.9961, Train-R2: 0.0052, Train-MAE: 0.8082\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "415/1000 - Train-loss: 0.9982, Train-R2: 0.0039, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "416/1000 - Train-loss: 0.9970, Train-R2: 0.0042, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "417/1000 - Train-loss: 0.9905, Train-R2: 0.0059, Train-MAE: 0.8075\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "418/1000 - Train-loss: 1.0008, Train-R2: 0.0038, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "419/1000 - Train-loss: 0.9950, Train-R2: 0.0033, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "420/1000 - Train-loss: 0.9970, Train-R2: 0.0045, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "421/1000 - Train-loss: 0.9921, Train-R2: 0.0050, Train-MAE: 0.8080\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "422/1000 - Train-loss: 1.0006, Train-R2: 0.0051, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "423/1000 - Train-loss: 0.9928, Train-R2: 0.0040, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "424/1000 - Train-loss: 0.9946, Train-R2: 0.0059, Train-MAE: 0.8077\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "425/1000 - Train-loss: 0.9944, Train-R2: 0.0029, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "426/1000 - Train-loss: 1.0021, Train-R2: 0.0026, Train-MAE: 0.8094\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "427/1000 - Train-loss: 1.0010, Train-R2: 0.0042, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "428/1000 - Train-loss: 0.9934, Train-R2: 0.0046, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "429/1000 - Train-loss: 0.9949, Train-R2: 0.0034, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "430/1000 - Train-loss: 0.9932, Train-R2: 0.0039, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "431/1000 - Train-loss: 0.9978, Train-R2: 0.0041, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "432/1000 - Train-loss: 0.9958, Train-R2: 0.0035, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "433/1000 - Train-loss: 1.0020, Train-R2: 0.0039, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "434/1000 - Train-loss: 0.9970, Train-R2: 0.0035, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "435/1000 - Train-loss: 0.9973, Train-R2: 0.0048, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "436/1000 - Train-loss: 0.9915, Train-R2: 0.0042, Train-MAE: 0.8083\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "437/1000 - Train-loss: 0.9953, Train-R2: 0.0043, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "438/1000 - Train-loss: 0.9946, Train-R2: 0.0046, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "439/1000 - Train-loss: 1.0006, Train-R2: 0.0035, Train-MAE: 0.8091\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8204\n",
      "440/1000 - Train-loss: 0.9953, Train-R2: 0.0028, Train-MAE: 0.8091\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "441/1000 - Train-loss: 0.9969, Train-R2: 0.0038, Train-MAE: 0.8085\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8204\n",
      "442/1000 - Train-loss: 0.9982, Train-R2: 0.0034, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "443/1000 - Train-loss: 1.0010, Train-R2: 0.0042, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "444/1000 - Train-loss: 0.9948, Train-R2: 0.0046, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "445/1000 - Train-loss: 0.9945, Train-R2: 0.0032, Train-MAE: 0.8096\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "446/1000 - Train-loss: 0.9981, Train-R2: 0.0048, Train-MAE: 0.8082\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "447/1000 - Train-loss: 0.9958, Train-R2: 0.0036, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "448/1000 - Train-loss: 0.9925, Train-R2: 0.0045, Train-MAE: 0.8080\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "449/1000 - Train-loss: 0.9946, Train-R2: 0.0048, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "450/1000 - Train-loss: 0.9939, Train-R2: 0.0030, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "451/1000 - Train-loss: 0.9998, Train-R2: 0.0028, Train-MAE: 0.8095\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "452/1000 - Train-loss: 0.9974, Train-R2: 0.0051, Train-MAE: 0.8082\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "453/1000 - Train-loss: 0.9972, Train-R2: 0.0030, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "454/1000 - Train-loss: 0.9949, Train-R2: 0.0038, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "455/1000 - Train-loss: 0.9973, Train-R2: 0.0040, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "456/1000 - Train-loss: 0.9960, Train-R2: 0.0032, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "457/1000 - Train-loss: 0.9960, Train-R2: 0.0029, Train-MAE: 0.8093\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "458/1000 - Train-loss: 0.9953, Train-R2: 0.0036, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "459/1000 - Train-loss: 1.0018, Train-R2: 0.0021, Train-MAE: 0.8092\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "460/1000 - Train-loss: 1.0052, Train-R2: 0.0019, Train-MAE: 0.8097\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "461/1000 - Train-loss: 0.9950, Train-R2: 0.0029, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "462/1000 - Train-loss: 0.9968, Train-R2: 0.0031, Train-MAE: 0.8091\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "463/1000 - Train-loss: 0.9953, Train-R2: 0.0033, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "464/1000 - Train-loss: 0.9940, Train-R2: 0.0035, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "465/1000 - Train-loss: 0.9954, Train-R2: 0.0049, Train-MAE: 0.8083\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "466/1000 - Train-loss: 0.9934, Train-R2: 0.0038, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "467/1000 - Train-loss: 0.9976, Train-R2: 0.0036, Train-MAE: 0.8091\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "468/1000 - Train-loss: 0.9928, Train-R2: 0.0042, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "469/1000 - Train-loss: 0.9958, Train-R2: 0.0042, Train-MAE: 0.8082\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "470/1000 - Train-loss: 0.9976, Train-R2: 0.0031, Train-MAE: 0.8092\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8207\n",
      "471/1000 - Train-loss: 0.9990, Train-R2: 0.0027, Train-MAE: 0.8094\t- Val-loss: 1.0979, Val-R2: 0.0029, Val-MAE: 0.8204\n",
      "472/1000 - Train-loss: 0.9949, Train-R2: 0.0033, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "473/1000 - Train-loss: 0.9996, Train-R2: 0.0041, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "474/1000 - Train-loss: 0.9964, Train-R2: 0.0023, Train-MAE: 0.8096\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "475/1000 - Train-loss: 0.9977, Train-R2: 0.0038, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "476/1000 - Train-loss: 0.9950, Train-R2: 0.0024, Train-MAE: 0.8097\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "477/1000 - Train-loss: 0.9994, Train-R2: 0.0037, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "478/1000 - Train-loss: 0.9956, Train-R2: 0.0032, Train-MAE: 0.8088\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "479/1000 - Train-loss: 0.9955, Train-R2: 0.0037, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8207\n",
      "480/1000 - Train-loss: 0.9943, Train-R2: 0.0046, Train-MAE: 0.8083\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "481/1000 - Train-loss: 0.9941, Train-R2: 0.0035, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8210\n",
      "482/1000 - Train-loss: 0.9967, Train-R2: 0.0022, Train-MAE: 0.8095\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "483/1000 - Train-loss: 0.9943, Train-R2: 0.0040, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "484/1000 - Train-loss: 0.9945, Train-R2: 0.0028, Train-MAE: 0.8092\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "485/1000 - Train-loss: 0.9992, Train-R2: 0.0024, Train-MAE: 0.8094\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8209\n",
      "486/1000 - Train-loss: 0.9986, Train-R2: 0.0028, Train-MAE: 0.8090\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "487/1000 - Train-loss: 0.9942, Train-R2: 0.0030, Train-MAE: 0.8091\t- Val-loss: 1.0978, Val-R2: 0.0031, Val-MAE: 0.8208\n",
      "488/1000 - Train-loss: 0.9930, Train-R2: 0.0034, Train-MAE: 0.8086\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "489/1000 - Train-loss: 0.9936, Train-R2: 0.0036, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "490/1000 - Train-loss: 0.9957, Train-R2: 0.0033, Train-MAE: 0.8092\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "491/1000 - Train-loss: 0.9934, Train-R2: 0.0045, Train-MAE: 0.8081\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "492/1000 - Train-loss: 0.9957, Train-R2: 0.0042, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "493/1000 - Train-loss: 0.9932, Train-R2: 0.0043, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "494/1000 - Train-loss: 0.9935, Train-R2: 0.0045, Train-MAE: 0.8087\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "495/1000 - Train-loss: 0.9954, Train-R2: 0.0028, Train-MAE: 0.8093\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "496/1000 - Train-loss: 0.9955, Train-R2: 0.0031, Train-MAE: 0.8089\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "497/1000 - Train-loss: 0.9978, Train-R2: 0.0023, Train-MAE: 0.8095\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8205\n",
      "498/1000 - Train-loss: 0.9941, Train-R2: 0.0040, Train-MAE: 0.8084\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8207\n",
      "499/1000 - Train-loss: 1.0033, Train-R2: 0.0052, Train-MAE: 0.8081\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "500/1000 - Train-loss: 0.9917, Train-R2: 0.0045, Train-MAE: 0.8085\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8208\n",
      "501/1000 - Train-loss: 0.9952, Train-R2: 0.0027, Train-MAE: 0.8094\t- Val-loss: 1.0978, Val-R2: 0.0030, Val-MAE: 0.8206\n",
      "\n",
      "Early stopping triggered after 501 epochs\n",
      "Best validation loss: 1.0876721640427907\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, val_loader, patience=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, target_scaler):\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in tqdm(test_loader, desc=\"Evaluating on test set\"):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            test_preds.extend(outputs.cpu().numpy())\n",
    "            test_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    test_preds = np.array(test_preds)\n",
    "    test_targets = np.array(test_targets)\n",
    "\n",
    "    # Inverse transform the predictions and targets if they were scaled\n",
    "    if target_scaler is not None:\n",
    "        test_preds = target_scaler.inverse_transform(test_preds.reshape(-1, 1)).flatten()\n",
    "        test_targets = target_scaler.inverse_transform(test_targets.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(test_targets, test_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(test_targets, test_preds)\n",
    "    r2 = r2_score(test_targets, test_preds)\n",
    "\n",
    "    # Convert frequencies back to periods\n",
    "    test_periods_pred = 24 / test_preds\n",
    "    test_periods_true = 24 / test_targets\n",
    "\n",
    "    # Calculate period-specific metrics\n",
    "    period_mae = mean_absolute_error(test_periods_true, test_periods_pred)\n",
    "    period_mse = mean_squared_error(test_periods_true, test_periods_pred)\n",
    "    period_rmse = np.sqrt(period_mse)\n",
    "    period_r2 = r2_score(test_periods_true, test_periods_pred)\n",
    "\n",
    "    return {\n",
    "        \"Frequency MSE\": mse,\n",
    "        \"Frequency RMSE\": rmse,\n",
    "        \"Frequency MAE\": mae,\n",
    "        \"Frequency R2\": r2,\n",
    "        \"Period MAE\": period_mae,\n",
    "        \"Period RMSE\": period_rmse,\n",
    "        \"Period R2\": period_r2,\n",
    "        \"Predictions\": test_preds,\n",
    "        \"True Values\": test_targets,\n",
    "        \"Period Predictions\": test_periods_pred,\n",
    "        \"True Periods\": test_periods_true,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on test set: 100%|██████████| 6/6 [00:00<00:00, 257.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency MSE: 2.9596\n",
      "Frequency RMSE: 1.7204\n",
      "Frequency MAE: 1.3901\n",
      "Frequency R2: 0.0065\n",
      "Period MAE: 4.9119\n",
      "Period RMSE: 8.1482\n",
      "Period R2: -0.1685\n",
      "Predictions: [3.4303386 3.2419407 3.4303422 3.4303393 3.4303439] ... [3.2433562 3.2103775 3.2103758 3.4303472 3.4303448]\n",
      "True Values: [1.5298998  3.8621905  8.243286   0.60228866 1.2255026 ] ... [3.4471862 3.4124355 2.9316916 1.3523716 4.6582766]\n",
      "Period Predictions: [6.9963937 7.402973  6.9963865 6.9963923 6.996383 ] ... [7.399742  7.4757566 7.4757605 6.9963765 6.9963813]\n",
      "True Periods: [15.687302   6.2140903  2.9114602 39.848003  19.583801 ] ... [ 6.9622    7.0331    8.186399 17.746601  5.15212 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(trained_model, test_loader, device, target_scaler)\n",
    "\n",
    "# Print the results\n",
    "for metric, value in results.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"{metric}: {value[:5]} ... {value[-5:]}\")\n",
    "    else:\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
